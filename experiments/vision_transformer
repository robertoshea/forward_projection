#@title Forward ViT training and evaluation functions
# <editor-fold desc="Forward transformer training and evaluation functions">

# Step 1: Extract image patches
def extract_patches(x, patch_size):
    n, num_channels = x.shape[:2]
    # Reshape into patches (n, num_patches, patch_size*patch_size*num_channels)
    patches = x.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)
    patches = patches.contiguous().view(n, num_channels, -1, patch_size * patch_size)
    patches = patches.permute(0, 2, 1, 3).contiguous().view(n, -1, patch_size * patch_size * num_channels)
    return patches  # Shape: (n, num_patches, patch_size*patch_size*num_channels)

# Multi-head attention
def scaled_dot_product_attention(Q, K, V):
    scale_factor = torch.sqrt(torch.tensor(Q.size(-1), dtype=torch.float32))
    scores = torch.matmul(Q, K.transpose(-2, -1)) / scale_factor
    attention_weights = nn.functional.softmax(scores, dim=-1)
    attention_output = torch.matmul(attention_weights, V)
    return attention_output

# forward conv1d function with parameter to prespecify label projection
def fit_w_conv1d(x_batches,
                 y_batches,
                 hidden_dim=32,
                 reg_factor=10.,
                 u=None,
                 activation="relu",
                 device=device,
                 training_method="forward_projection",
                 return_qu=False, ):
    x_channels = x_batches[0].shape[-1]
    y_channels = y_batches[0].shape[-1]

    q = torch.randn((x_channels, hidden_dim), device=device)  # data projection matrix
    if u is None:
        u = torch.randn((y_channels, hidden_dim), device=device)  # label projection matrix
    z_batches = []  # batches of targets
    for x_i, y_i in zip(x_batches, y_batches):

        x_i = x_i.to(device)
        y_i = y_i.to(device)

        # project labels
        y_proj = torch.sign(y_i @ u)
        match training_method:
            case "forward_projection":
                x_proj = torch.sign(x_i @ q)
            case "label_projection":
                x_proj = torch.zeros(x_i.shape[:-1] + (u.shape[-1],),
                                     device=device)
            case "noisy_label_projection":
                x_proj = torch.sign(torch.randn(x_i.shape[:-1] + (u.shape[-1],),
                                                device=device))

        # generate target potentials (z)
        z_i = x_proj + y_proj

        # transpose distribution of labels
        if activation_shift_dict[activation] != 0:
            z_i += activation_shift_dict[activation]
        if activation_rescale_dict[activation] != 1:
            z_i *= activation_rescale_dict[activation]
        z_batches.append(z_i.to("cpu"))

    # model target potentials
    w = ridge_regression_w_conv1d(x_batches, z_batches, reg_factor=reg_factor)

    if return_qu:
        return w, q, u
    else:
        return w, None, None


# multi-head attention fitting function
def fit_multi_head_attention(x_batches,
                             y_batches,
                             num_heads,
                             head_dim,
                             ):
    y_channels = y_batches[0].shape[-1]
    total_dim = head_dim * num_heads
    u = torch.randn((y_channels, total_dim), device=device)  # label projection matrix
    w_qkv = []
    for _ in range(3):
        # use different q projection matrices for fitting query key and value weights
        w_i, _, _ = fit_w_conv1d(x_batches=x_batches,
                                 y_batches=y_batches,
                                 hidden_dim=total_dim,
                                 u=u)
        w_i = w_i.reshape((w_i.shape[0], head_dim, num_heads))
        w_qkv.append(w_i)
    w_query, w_key, w_value = w_qkv

    return w_query, w_key, w_value


def multi_head_attention(x_batches,
                         w_query,
                         w_key,
                         w_value):
    num_heads = w_query.shape[-1]
    for i in range(len(x_batches)):
        x_i = x_batches[i]
        concatenated_heads = []
        for head_i in range(num_heads):
            Q = x_i @ w_query[..., head_i]
            K = x_i @ w_key[..., head_i]
            V = x_i @ w_value[..., head_i]
            attention_i = scaled_dot_product_attention(Q, K, V)
            concatenated_heads.append(attention_i)
        concatenated_heads = torch.cat(concatenated_heads, dim=-1)
        x_batches[i] = concatenated_heads
    return x_batches

def add_positional_encoding(x, positional_encoding=None):
    pos_encoding_long = positional_encoding.repeat(x.shape[0], 1, 1).to(device)
    x = torch.concatenate([x, pos_encoding_long], dim=-1)
    return x  # Shape: (batch_size, num_patches, embed_dim+1)


# Step 6: Add and Normalize layers
def add_and_normalize(x, y):
    return nn.functional.layer_norm(x + y, x.size())


def train_forward_transformer(x,
                              y,
                              patch_size,
                              num_heads,
                              embed_dim=128,
                              mlp_dim=128,
                              n_attn_layers=1,
                              batch_size=100,
                              activation="relu",
                              global_layer="average",
                              reg_factor=10.,
                              training_method="forward_projection",
                              verbose=False,
                              device=device):
    """
    Train a transformer model using ridge regression for weights.

    Args:
        x (torch.Tensor): Input tensor of shape (n_samples, height, width, channels).
        y (torch.Tensor): Target tensor of shape (n_samples, num_classes).
        patch_size (int): Size of patches to extract.
        num_heads (int): Number of attention heads.
        head_dim (int): Dimension of each attention head.
        n_attn_layers (int): Number of attention layers.
        mlp_dim (int): Dimension of the MLP layer.
        batch_size (int): Batch size for training.
        activation (str): Activation function to use.
        reg_factor (float): Regularization factor for ridge regression.
        return_qu (bool): Whether to return projection matrices.
        verbose (bool): Whether to print progress.
        device (str): Device to use for training.

    Returns:
        dict: Dictionary containing trained weights and training time.
    """
    with torch.no_grad():
        if y.ndim == 2:
            y = torch.unsqueeze(y, dim=1)

        start_time = time.perf_counter()
        activation_fn = activation_dict[activation]
        height, width = x.shape[1:3]
        num_patches_height = height // patch_size

        # Step 1: Extract patches
        x = x.permute(0, 3, 1, 2)  # Convert to (n, channels, height, width)
        x = extract_patches(x, patch_size)

        # Step 2: Add positional encoding
        positional_encoding = torch.linspace(-1, 1, num_patches_height)[:, None].repeat((1, num_patches_height))
        positional_encoding = torch.stack([positional_encoding.T, positional_encoding])
        positional_encoding = positional_encoding.flatten(start_dim=1).T[None, ...]
        x = add_positional_encoding(x, positional_encoding)

        # Step 3: Split into batches
        x_batches = list(torch.split(x, split_size_or_sections=batch_size))
        y_batches = list(torch.split(y, split_size_or_sections=batch_size))

        # Step 4: Fit embedding layer
        w_embedding, _, _ = fit_w_conv1d(x_batches,
                                         y_batches,
                                         hidden_dim=embed_dim,
                                         activation="relu")
        x_batches = [activation_fn(x_i @ w_embedding) for x_i in x_batches]

        # Initialize weights
        w_query_list, w_key_list, w_value_list, w_mlp_list = [], [], [], []

        # Step 5: Fit attention layers
        for layer in range(n_attn_layers):
            if verbose:
                print(f"Training attention layer {layer + 1}/{n_attn_layers}")

            # Fit multi-head attention weights
            if training_method == "random":
                w_query = torch.randn((x_batches[0].shape[-1], num_heads, head_dim), device=device)
                w_key = torch.randn((x_batches[0].shape[-1], num_heads, head_dim), device=device)
                w_value = torch.randn((x_batches[0].shape[-1], num_heads, head_dim), device=device)
                w_query /= w_query.norm(dim=-1, keepdim=True)
                w_key /= w_key.norm(dim=-1, keepdim=True)
                w_value /= w_value.norm(dim=-1, keepdim=True)

            if training_method == "forward_projection":
                w_query, w_key, w_value = fit_multi_head_attention(
                    x_batches, y_batches, num_heads, head_dim
                )
            w_query_list.append(w_query)
            w_key_list.append(w_key)
            w_value_list.append(w_value)

            # Apply multi-head attention
            x_batches = multi_head_attention(x_batches, w_query, w_key, w_value)

            # Fit MLP weights
            x_batches = [concatenate_ones(x_i) for x_i in x_batches]
            if training_method == "random":
                w_mlp = torch.randn((x_batches[0].shape[-1], mlp_dim), device=device)
                w_mlp /= w_mlp.norm(dim=-1, keepdim=True)
            if training_method == "forward_projection":
                w_mlp, _, _ = fit_w_conv1d(x_batches, y_batches, hidden_dim=mlp_dim, activation=activation)
            w_mlp_list.append(w_mlp)
            x_batches = [activation_fn(x_i @ w_mlp) for x_i in x_batches]

        # Step 6: Fit output layer
        if verbose:
            print("Fitting output layer")
        for i in range(len(x_batches)):
            x_batches[i] = concatenate_ones(x_batches[i])
            if global_layer == "flatten":
                x_batches[i] = x_batches[i].flatten(start_dim=1)[:, None, :]
            else:
                x_batches[i] = torch.mean(x_batches[i], dim=1, keepdim=True)
            y_batches[i] = 2 * y_batches[i] - 1
        w_out = ridge_regression_w_conv1d(x_batches, y_batches, reg_factor=reg_factor)

        end_time = time.perf_counter()
        training_time = end_time - start_time

        w_list = [w_embedding, w_query_list, w_key_list, w_value_list, w_mlp_list, w_out]

        # Return weights and training time
        return w_list, training_time


def evaluate_forward_transformer(x,
                                 y,
                                 w_list,
                                 patch_size,
                                 activation="relu",
                                 batch_size=100,
                                 global_layer="average",
                                 device="cpu"):
    """
    Evaluate a trained transformer model.

    Args:
        x (torch.Tensor): Input tensor of shape (n_samples, height, width, channels).
        y (torch.Tensor): Target tensor of shape (n_samples, num_classes).
        w_query_list (list): List of query weight matrices for each attention layer.
        w_key_list (list): List of key weight matrices for each attention layer.
        w_value_list (list): List of value weight matrices for each attention layer.
        w_mlp_list (list): List of MLP weight matrices for each layer.
        w_out (torch.Tensor): Output layer weight matrix.
        patch_size (int): Size of patches to extract.
        activation (str): Activation function to use.
        batch_size (int): Batch size for evaluation.
        global_layer (str): Global pooling method ("average" or "flatten").
        device (str): Device to use for evaluation.

    Returns:
        torch.Tensor: Evaluation metrics.
    """
    w_embedding, w_query_list, w_key_list, w_value_list, w_mlp_list, w_out = w_list

    with torch.no_grad():
        if y.ndim == 2:
            y = torch.unsqueeze(y, dim=1)

        start_time = time.perf_counter()
        activation_fn = activation_dict[activation]
        height, width = x.shape[1:3]
        num_patches_height = height // patch_size

        # Step 1: Extract patches
        x = x.permute(0, 3, 1, 2)  # Convert to (n, channels, height, width)
        x = extract_patches(x, patch_size)

        # Step 2: Add positional encoding
        positional_encoding = torch.linspace(-1, 1, num_patches_height)[:, None].repeat((1, num_patches_height))
        positional_encoding = torch.stack([positional_encoding.T, positional_encoding])
        positional_encoding = positional_encoding.flatten(start_dim=1).T[None, ...]
        x = add_positional_encoding(x, positional_encoding)

        # Step 3: Split into batches
        x_batches = list(torch.split(x, split_size_or_sections=batch_size))
        y_batches = list(torch.split(y, split_size_or_sections=batch_size))

        # Step 4: embedding layer
        x_batches = [activation_fn(x_i @ w_embedding) for x_i in x_batches]

        # Step 5: attention layers
        n_attn_layers = len(w_query_list)
        for layer in range(n_attn_layers):
            # Apply multi-head attention
            w_query = w_query_list[layer]
            w_key = w_key_list[layer]
            w_value = w_value_list[layer]
            x_batches = multi_head_attention(x_batches, w_query, w_key, w_value)

            # Apply MLP weights
            x_batches = [concatenate_ones(x_i) for x_i in x_batches]
            w_mlp = w_mlp_list[layer]
            x_batches = [activation_fn(x_i @ w_mlp) for x_i in x_batches]

        # Step 6: Apply output layer
        for i in range(len(x_batches)):
            x_batches[i] = concatenate_ones(x_batches[i])
            if global_layer == "flatten":
                x_batches[i] = x_batches[i].flatten(start_dim=1)[:, None, :]
            else:
                x_batches[i] = torch.mean(x_batches[i], dim=1, keepdim=True)
            y_batches[i] = 2 * y_batches[i] - 1
        yhat = [x_i @ w_out for x_i in x_batches]
        yhat = torch.concatenate(yhat)
        metrics = compute_metrics(yhat, y)

    return metrics


# </editor-fold>

#@title Forward ViT experiments
# <editor-fold desc="Forward ViT experiments">
model_parameters = expand_grid({
    'fold': list(range(5)),
    'training_method': ["forward_projection", "random", ],
    'activation': ["relu"],
})
verbose = True

seed = 0
random.seed(seed)
torch.manual_seed(seed)
np.random.seed(seed)
forward_conv1d_experiments = []
dataset_i = "CIFAR10"
n_classes = 2
print(dataset_i)
X_trainval, Y_trainval, X_test, Y_test, folds = load_dataset(dataset_i)
selected_idx = Y_trainval[:, :n_classes].any(dim=(1,))
X_trainval = X_trainval[selected_idx]
Y_trainval = Y_trainval[selected_idx][:, :n_classes]
folds = folds[selected_idx]
selected_idx = Y_test[:, :n_classes].any(dim=(1,))
X_test = X_test[selected_idx]
Y_test = Y_test[selected_idx][:, :n_classes]

# Set parameters
verbose = True
batch_size = 50
patch_size = 4
num_heads = 8
embed_dim = 64
head_dim = embed_dim // num_heads
n_ViT_layers = 4
mlp_dim = 64
global_layer = "average"
attn_softmax = True
experiments = []
for model_parameters_i in range(len(model_parameters)):

    print(model_parameters_i)

    training_method = model_parameters.training_method[model_parameters_i]
    fold = model_parameters.fold[model_parameters_i]
    activation = model_parameters.activation[model_parameters_i]

    train_folds = folds != fold
    val_folds = torch.logical_not(train_folds)
    X_train, X_val = X_trainval[train_folds], X_trainval[val_folds]
    Y_train, Y_val = Y_trainval[train_folds], Y_trainval[val_folds]

    w_list, training_time = train_forward_transformer(x=X_trainval,
                                                      y=Y_trainval,
                                                      training_method=training_method,
                                                      activation=activation,
                                                      n_attn_layers=n_ViT_layers,
                                                      device=device,
                                                      batch_size=10,
                                                      patch_size=patch_size,
                                                      num_heads=num_heads,
                                                      global_layer=global_layer
                                                      )
    train_metrics = evaluate_forward_transformer(x=X_train,
                                                 y=Y_train,
                                                 w_list=w_list,
                                                 patch_size=patch_size,
                                                 device=device,
                                                 activation=activation,
                                                 global_layer=global_layer)
    val_metrics = evaluate_forward_transformer(x=X_val,
                                               y=Y_val,
                                               w_list=w_list,
                                               patch_size=patch_size,
                                               device=device,
                                               activation=activation,
                                               global_layer=global_layer)
    test_metrics = evaluate_forward_transformer(x=X_test,
                                                y=Y_test,
                                                w_list=w_list,
                                                patch_size=patch_size,
                                                device=device,
                                                activation=activation,
                                                global_layer=global_layer)
    if verbose:
        print(training_method)
        print(train_metrics)
        print(val_metrics)
        print(test_metrics)

    out = {
        'dataset': "CIFAR",
        'training_method': training_method,
        'fold': fold,
        'activation': activation,
        "embed_dim" : embed_dim,
        "mlp_dim" : mlp_dim,
        "n_ViT_layers" : n_ViT_layers,
        "num_heads" : num_heads,
        "patch_size" : patch_size,
        'train_auc': train_metrics[0].item(),
        'train_acc': train_metrics[1].item(),
        'train_prec': train_metrics[2].item(),
        'train_recall': train_metrics[3].item(),
        'train_f1': train_metrics[4].item(),
        'val_auc': val_metrics[0].item(),
        'val_acc': val_metrics[1].item(),
        'val_prec': val_metrics[2].item(),
        'val_recall': val_metrics[3].item(),
        'val_f1': val_metrics[4].item(),
        'test_auc': test_metrics[0].item(),
        'test_acc': test_metrics[1].item(),
        'test_prec': test_metrics[2].item(),
        'test_recall': test_metrics[3].item(),
        'test_f1': test_metrics[4].item(),
        'training_time': training_time,
    }
    experiments.append(out)
experiments = pd.DataFrame(experiments)
output_file = os.path.join(output_dir, "forward_vit_experiments.csv")
experiments.to_csv(path_or_buf=output_file)

# </editor-fold>

#@title SGD ViT training and evaluation functions
# <editor-fold desc="SGD ViT training and evaluation functions">

class PatchEmbedding(nn.Module):
    def __init__(self, img_size, patch_size, in_channels, embed_dim):
        super().__init__()
        self.n_patches = (img_size // patch_size) ** 2
        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        x = self.proj(x)  # (B, embed_dim, H/patch, W/patch)
        x = x.flatten(2)  # (B, embed_dim, N_patches)
        x = x.transpose(1, 2)  # (B, N_patches, embed_dim)
        return x

class ViTLayer(nn.Module):
    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.):
        super().__init__()
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, mlp_dim),
            nn.ReLU(),
            nn.Linear(mlp_dim, embed_dim),
        )

    def forward(self, x):
        # Self-attention block
        x2 = self.norm1(x)
        attn_out, _ = self.attn(x2, x2, x2)
        x = x + attn_out
        # MLP block
        x2 = self.norm2(x)
        x = x + self.mlp(x2)
        return x

def create_positional_encoding(x, patch_size):
    num_patches_height = x.shape[2] // patch_size
    positional_encoding = torch.linspace(-1, 1, num_patches_height)[:, None].repeat((1, num_patches_height))
    positional_encoding = torch.stack([positional_encoding.T, positional_encoding])
    positional_encoding = positional_encoding.flatten(start_dim=1).T[None, ...]

    return positional_encoding


class ViTPositionalEncoding(nn.Module):
    def __init__(self, num_patches, embed_dim):
        super().__init__()
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
        nn.init.trunc_normal_(self.pos_embed, std=0.02)

    def forward(self, x):
        # x: (batch_size, num_patches, embed_dim)
        return x + self.pos_embed

class ViTModel(nn.Module):
    def __init__(self,
                 embed_dim=64,
                 mlp_dim=64,
                 n_ViT_layers=4,
                 patch_size=4,
                 num_heads=8,
                 in_channels=3,
                img_size=32,
                 n_classes=2,
                 training_method="backprop",
                 ):
        super().__init__()
        self.patch_size = patch_size
        self.in_channels = in_channels
        self.training_method = training_method
        self.n_classes = n_classes
        self.pos_encoding = ViTPositionalEncoding((img_size // patch_size) ** 2, embed_dim)
        self.patch_embedding = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)
        self.vit_layers = [ViTLayer(embed_dim, num_heads, mlp_dim) for _ in range(n_ViT_layers)]
        self.output_layer = nn.Linear(embed_dim, n_classes)
        self.all_layers = nn.ModuleList(self.vit_layers + [self.output_layer])
        self.opt = torch.optim.Adam(self.all_layers.parameters())


    def forward(self, x):
        x = self.patch_embedding(x)
        x = self.pos_encoding(x)
        for layer_l in self.vit_layers:
            x = layer_l(x)
        x = torch.mean(x, dim=1)
        yhat = self.output_layer(x)
        return yhat

def train_sgd_vit(X_train,
                     Y_train,
                     X_val,
                     Y_val,
                     patience=5,
                     max_epochs=25,
                     batch_size=25,
                  embed_dim=64,
                 mlp_dim=64,
                  num_heads=8,
                 n_ViT_layers=4,
                  patch_size=4,
                     verbose=False,
                     device=device,
                     training_method="backprop",
                     loss_fn=torch.nn.CrossEntropyLoss()):
    torch.cuda.empty_cache()
    start_time = time.perf_counter()

    model = ViTModel(
        n_classes=Y_train.shape[1],
        num_heads=num_heads,
        patch_size=patch_size,
        embed_dim=embed_dim,
        mlp_dim=mlp_dim,
        n_ViT_layers=n_ViT_layers,
        training_method=training_method,
    ).to(device)
    train_loss = []
    val_loss = []
    best_val_loss = torch.inf
    patience_counter = 0
    for epoch_i in range(max_epochs):
        train_loss_i = train_sgd(model=model,
                                 x=X_train,
                                 y=Y_train,
                                 loss_fn=loss_fn,
                                 batch_size=batch_size
                                 )
        train_loss.append(train_loss_i.item())
        val_loss_i = validate_sgd(model=model,
                                  x=X_val,
                                  y=Y_val,
                                  loss_fn=loss_fn,
                                  batch_size=batch_size)
        val_loss.append(val_loss_i.item())
        if verbose:
            print(val_loss_i)
        if val_loss_i < best_val_loss:
            best_val_loss = val_loss_i
            patience_counter = 0
        else:
            patience_counter += 1
        if patience_counter == (patience - 1):
            break

    end_time = time.perf_counter()
    training_time = end_time - start_time
    training_epochs = epoch_i

    model = ViTModel().to(device)
    print("final training")
    X_trainval = torch.concatenate([X_train, X_val])
    Y_trainval = torch.concatenate([Y_train, Y_val])
    for _ in range(epoch_i):
        _ = train_sgd(model=model,
                      x=X_trainval,
                      y=Y_trainval,
                      loss_fn=loss_fn,
                      batch_size=batch_size)

    return model, training_time, training_epochs

# </editor-fold>

#@title SGD ViT experiments
# <editor-fold desc="SGD ViT experiments">

model_parameters = expand_grid({
    'fold': list(range(5)),
    'activation': ["relu"],
    'training_method': ["backprop",],
})

verbose = False

seed = 0
random.seed(seed)
torch.manual_seed(seed)
np.random.seed(seed)
experiments = []
dataset_i = "CIFAR10"

print(dataset_i)
X_trainval, Y_trainval, X_test, Y_test, folds = load_dataset(dataset_i,
                                                              channels_last=False)
n_classes
selected_idx = Y_trainval[:, :n_classes].any(dim=(1,))
X_trainval = X_trainval[selected_idx]
Y_trainval = Y_trainval[selected_idx][:, :n_classes]
folds = folds[selected_idx]
selected_idx = Y_test[:, :n_classes].any(dim=(1,))
X_test = X_test[selected_idx]
Y_test = Y_test[selected_idx][:, :n_classes]

for model_parameters_i in range(len(model_parameters)):
    print(model_parameters_i)

    fold = model_parameters.fold[model_parameters_i]
    activation = model_parameters.activation[model_parameters_i]
    training_method = model_parameters.training_method[model_parameters_i]
    embed_dim = 64
    mlp_dim = 64
    n_ViT_layers = 4
    num_heads = 8
    patch_size = 4

    train_folds = folds != fold
    val_folds = torch.logical_not(train_folds)
    X_train, X_val = X_trainval[train_folds], X_trainval[val_folds]
    Y_train, Y_val = Y_trainval[train_folds], Y_trainval[val_folds]

    model, training_time, training_epochs = train_sgd_vit(
        X_train=X_train,
        Y_train=Y_train,
        X_val=X_val,
        Y_val=Y_val,
        max_epochs=50,
        batch_size=25,
        patch_size=patch_size,
        embed_dim=embed_dim,
        mlp_dim=mlp_dim,
        n_ViT_layers=n_ViT_layers,
        num_heads=num_heads,
        verbose=True)

    train_metrics = evaluate_sgd(model=model,
                                  x=X_train,
                                  y=Y_train,
                                  )

    val_metrics = evaluate_sgd(model=model,
                                x=X_val,
                                y=Y_val,
                                )

    test_metrics = evaluate_sgd(model=model,
                                x=X_test,
                                y=Y_test,
                                )
    if verbose:
        print(training_method, dataset_i, activation, sep="\n")
        print(train_metrics)
        print(val_metrics)
        print(test_metrics)

    out = {
        'dataset': "CIFAR",
        'training_method': training_method,
        'fold': fold,
        'activation': activation,
        "embed_dim" : embed_dim,
        "mlp_dim" : mlp_dim,
        "n_ViT_layers" : n_ViT_layers,
        "num_heads" : num_heads,
        "patch_size" : patch_size,
        'train_auc': train_metrics[0].item(),
        'train_acc': train_metrics[1].item(),
        'train_prec': train_metrics[2].item(),
        'train_recall': train_metrics[3].item(),
        'train_f1': train_metrics[4].item(),
        'val_auc': val_metrics[0].item(),
        'val_acc': val_metrics[1].item(),
        'val_prec': val_metrics[2].item(),
        'val_recall': val_metrics[3].item(),
        'val_f1': val_metrics[4].item(),
        'test_auc': test_metrics[0].item(),
        'test_acc': test_metrics[1].item(),
        'test_prec': test_metrics[2].item(),
        'test_recall': test_metrics[3].item(),
        'test_f1': test_metrics[4].item(),
        'training_time': training_time,
        'training_epochs': training_epochs,
    }
    experiments.append(out)

experiments = pd.DataFrame(experiments)
output_file = os.path.join(output_dir, f"sgd_vit_experiments.csv")
experiments.to_csv(path_or_buf=output_file)

# </editor-fold>
