
# <editor-fold desc="load libraries">
import numpy as np
import os
import pandas as pd
import torchvision
import torch
import torch.nn as nn
import torchmetrics
import json
from itertools import product
from torchvision import datasets
from torchvision.transforms import ToTensor
from datetime import date
import time
import random
import wfdb

import matplotlib.pyplot as plt
import matplotlib

matplotlib.rcParams["mathtext.fontset"] = "cm"

device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)
print(f"Using {device} device")

from genomic_benchmarks.dataset_getters.pytorch_datasets import get_dataset


# </editor-fold>
#@title sgd mlp functions
# <editor-fold desc="SGD mlp training and evaluation functions">

class mlpModel(nn.Module):
    def __init__(self,
                 training_method,
                 activation_fn,
                 in_features=None,
                 hidden_dims=None,
                 num_classes=None,
                 ):
        super(mlpModel, self).__init__()
        self.activation_fn = activation_fn
        self.in_features = [in_features] + hidden_dims
        if training_method == "forward_forward":
            self.in_features[0] += num_classes
        self.out_features = hidden_dims + [num_classes]
        self.layers = nn.ModuleList(
            [nn.Linear(in_l, out_l) for in_l, out_l in zip(self.in_features, self.out_features)])
        self.training_method = training_method
        self.detach_output = training_method != "backprop"
        if training_method == "backprop":
            self.opt = torch.optim.Adam(self.layers.parameters(), )
        if training_method == "local_supervision":
            self.opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.layers]
        if training_method == "forward_forward":
            self.opts = [torch.optim.Adam(layer_l.parameters(),
                                          lr=0.03) for layer_l in self.layers]
        if training_method == "local_supervision":
            self.ls_layers = nn.ModuleList([nn.Linear(out_l, num_classes) for out_l in hidden_dims])
        if training_method == "predictive_coding":
            self.back_layers = nn.ModuleList(
                [nn.Linear(out_l, in_l) for in_l, out_l in zip(self.in_features[:-1], self.out_features[:-1])])
            self.opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.layers]
            self.back_opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.back_layers]
        if training_method == "difference_target_propagation":
            self.opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.layers]
            self.back_layers = nn.ModuleList(
                [nn.Linear(in_l, out_l) for in_l, out_l in zip(self.out_features, self.in_features)])
            self.back_opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.back_layers]
        if training_method == "stochastic_forward_projection":
            self.Q = nn.ModuleList(
                [nn.Linear(in_l, out_l) for in_l, out_l in zip(self.in_features[:-1], self.out_features[:-1])])
            self.U = nn.ModuleList(
                [nn.Linear(num_classes, out_l) for out_l in self.out_features[:-1]])
            self.opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.layers]

    def forward(self, x):
        for layer_l in self.layers[:-1]:
            x = layer_l(x)
            x = self.activation_fn(x)
        if self.detach_output:
            x = torch.detach(x)
        x = self.layers[-1](x)
        return x

    def forward_ls(self, x):
        yhats = []
        for layer_l, ls_layer_l in zip(self.layers[:-1], self.ls_layers):
            x = layer_l(x)
            x = self.activation_fn(x)
            yhat_l = ls_layer_l(x)
            yhats.append(yhat_l)
            x = torch.detach(x)
        yhats.append(self.layers[-1](x))
        return yhats

    def forward_ff(self, xy):
        goodnesses = []
        for layer_l in self.layers[:-1]:
            xy = xy / (xy.norm(dim=1, keepdim=True) + 0.001)
            xy = layer_l(xy)
            xy = self.activation_fn(xy)
            goodness_l = xy.square().mean(dim=1)
            goodnesses.append(goodness_l)
            xy = torch.detach(xy)
        return goodnesses

    def forward_pc(self, x):
        local_losses = []
        for l in range(len(self.layers) - 1):
            x_out = self.layers[l](x)
            x_out = self.activation_fn(x_out)
            xhat = self.back_layers[l](x_out)
            xhat = self.activation_fn(xhat)
            loss_l = torch.mean((xhat - x) ** 2)
            local_losses.append(loss_l)
            x = x_out.detach()
        yhat = self.layers[-1](x)
        return local_losses, yhat

    def forward_dtp(self, x):

        h_list = []
        for l in range(len(self.layers) - 1):
            x = self.layers[l](x)
            x = self.activation_fn(x)
            h_list.append(x)
            x = x.detach()
        yhat = self.layers[-1](x)
        h_list.append(yhat)

        hhat_list = [None] * (len(h_list))
        hhat_list[-1] = h_list[-1].detach()
        for l in list(range(1, len(h_list))):
            h_curr = h_list[-l].detach()
            h_prev = h_list[-(l+1)].detach()
            hhat_curr = hhat_list[-l].detach()
            ghhat_curr = self.back_layers[-l](hhat_curr)
            ghhat_curr = self.activation_fn(ghhat_curr)
            gh_curr = self.back_layers[-l](h_curr)
            gh_curr = self.activation_fn(gh_curr)
            hhat_prev = h_prev + ghhat_curr - gh_curr
            hhat_list[-(l+1)] = hhat_prev
        local_losses = []
        for l in range(len(h_list)-1):
            loss_l = torch.mean((hhat_list[l] - h_list[l]) ** 2)
            local_losses.append(loss_l)

        return local_losses, yhat

    def forward_fp(self, x, y):
        local_losses = []
        for l in range(len(self.layers) - 1):
            x_out = self.layers[l](x)
            xq = torch.sign(self.Q[l](x.detach()))
            yu = torch.sign(self.U[l](y.detach()))
            target = xq + yu
            loss_l = torch.mean((x_out - target) ** 2)
            local_losses.append(loss_l)
            x_out = self.activation_fn(x_out)
            x = x_out.detach()
        yhat = self.layers[-1](x)

        return local_losses, yhat




def train_sgd_mlp(X_train,
                  Y_train,
                  X_val,
                  Y_val,
                  activation,
                  training_method,
                  hidden_dims=[1000] * 3,
                  patience=5,
                  max_epochs=100,
                  batch_size=50,
                  verbose=False,
                  loss_fn=torch.nn.CrossEntropyLoss()):
    torch.cuda.empty_cache()
    start_time = time.perf_counter()
    activation_fn = activation_dict[activation]

    in_features = X_train.shape[1]
    model = mlpModel(
        training_method=training_method,
        in_features=in_features,
        hidden_dims=hidden_dims,
        num_classes=Y_train.shape[1],
        activation_fn=activation_fn,
    ).to(device)
    train_loss = []
    val_loss = []
    best_val_loss = torch.inf
    patience_counter = 0
    for epoch_i in range(max_epochs):
        train_loss_i = train_sgd(model=model,
                                 x=X_train,
                                 y=Y_train,
                                 loss_fn=loss_fn,
                                 batch_size=batch_size)
        train_loss.append(train_loss_i.item())
        val_loss_i = validate_sgd(model=model,
                                  x=X_val,
                                  y=Y_val,
                                  loss_fn=loss_fn,
                                  batch_size=batch_size)
        val_loss.append(val_loss_i.item())
        if verbose:
            print(val_loss_i)
        if val_loss_i < best_val_loss:
            best_val_loss = val_loss_i
            patience_counter = 0
        else:
            patience_counter += 1
        if patience_counter == (patience - 1):
            break

    end_time = time.perf_counter()
    training_time = end_time - start_time
    training_epochs = epoch_i

    return model, training_time, training_epochs

# </editor-fold>
