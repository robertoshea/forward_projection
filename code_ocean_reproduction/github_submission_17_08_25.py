# -*- coding: utf-8 -*-
"""github submission 17_08_25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/100CO-cx-tTbywp-110dMy4m3F30wuZgM
"""

# @title data download link
# <editor-fold desc="data download link">

"""
All preprocessed study datasets are available from https://data.mendeley.com/datasets/fb7xddyxs4/2
Data may be downloaded either manually, or using the auto-download function below
Data should be unzipped into the data_dir below

after running code, all

"""

import os
import urllib
import zipfile

timenow = datetime.now().strftime("%Y-%m-%d %H%M%S")
data_dir = f"data_{timenow}" ##datasets to be unzipped here
os.mkdir(data_dir)
output_dir = f"results_{timenow}" ### result tables will be saved here in .csv format
os.mkdir(output_dir)

#function to auto download data
auto_download = False # set to True to auto-download
if auto_download:
  import urllib
  import zipfile

  url = "https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/fb7xddyxs4-2.zip"
  zip_path, _ = urllib.request.urlretrieve(url)
  with zipfile.ZipFile(zip_path, "r") as f:
      f.extractall(data_dir)


# </editor-fold>

# @title load libraries

# <editor-fold desc="load libraries">

#set to true to install torchmetrics if using google colab
using_google_colab = False
using_google_drive = False

if using_google_colab:
  !pip install torchmetrics

if using_google_drive:
  from google.colab import drive
  drive.mount('/content/drive')


import numpy as np
import os
import pandas as pd
import torch
import torch.nn as nn
import torchmetrics
import json
from itertools import product
from datetime import date, datetime
import time
import random
import zipfile

import matplotlib.pyplot as plt
import matplotlib

matplotlib.rcParams["mathtext.fontset"] = "cm"

device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)
print(f"Using {device} device")




# </editor-fold>

# @title utility functions
# <editor-fold desc="utility functions">



def expand_grid(dictionary):
    return pd.DataFrame([row for row in product(*dictionary.values())],
                        columns=dictionary.keys())

def identity_func(x):
    return x


def rec_listdir(dir):
    paths = []
    for root, directories, filenames in os.walk(dir):
        for filename in filenames:
            paths.append(os.path.join(root, filename))
    return paths


def compute_metrics(yhat, y):
    y = torch.squeeze(y).to('cpu')
    yhat = torch.squeeze(yhat).to('cpu')
    num_classes = y.shape[1]
    y = torch.argmax(y, dim=1)
    yhat_argmax = torch.argmax(yhat, dim=1)
    auc = torchmetrics.AUROC(task="multiclass", num_classes=num_classes, average="macro")(yhat, y)
    acc = torchmetrics.Accuracy(task="multiclass", num_classes=num_classes, average="macro")(yhat_argmax, y)
    recall = torchmetrics.Recall(task="multiclass", num_classes=num_classes, average="macro")(yhat_argmax, y)
    prec = torchmetrics.Precision(task="multiclass", num_classes=num_classes, average="macro")(yhat_argmax, y)
    f1 = torchmetrics.F1Score(task="multiclass", num_classes=num_classes, average="macro")(yhat_argmax, y)

    out = torch.stack([auc, acc, recall, prec, f1])

    return out


def expand_grid(dictionary):
    return pd.DataFrame([row for row in product(*dictionary.values())],
                        columns=dictionary.keys())


def standardise_fn(x, dim=0):
    x_ = torch.clone(x.detach())
    mu = x_.mean(dim=dim, keepdims=True)
    x_ -= mu
    s = x_.std(dim=dim, keepdims=True) + 1 / len(x_)
    x_ /= s
    return x_


def mean_sd_func(x):
    mu = x.mean()
    sd = x.std()
    out = f"{mu:.2f} \u00B1 {sd:.3f}"
    return out


def mean_sd_func2(x):
    mu = (x * 100).mean()
    sd = (x * 100).std()
    out = f"{mu:.1f} \u00B1 {sd:.1f}"
    return out

def mean_sd_func3(x):
    mu = x.mean()
    sd = x.std()
    out = f"{mu:.1f} \u00B1 {sd:.1f}"
    return out


ACGT_mapping = dict(zip("ACGTN", range(5)))
ACGT_eye = torch.eye(5, dtype=torch.bool)[:, :4]


def dna_to_onehot(x):
    x_ = [ACGT_mapping[i] for i in x]
    x_ = ACGT_eye[x_]
    return x_


date_today = date.today().strftime("%d_%m_%y")
if not os.path.exists("tables"):
    os.mkdir('tables')

# </editor-fold>

# @title data loading function



def load_dataset(dataset_i, data_dir=data_dir, channels_last=True):
    output_name = os.path.join(data_dir, f"{dataset_i}_data.npz")
    if not os.path.exists(output_name):
        raise FileNotFoundError(f"Dataset {dataset_i} not found in {output_name}")
    npzfile = np.load(output_name)
    X_trainval, Y_trainval, X_test, Y_test, folds =  [torch.tensor(npzfile[i], dtype=torch.float32, device=device) for i in npzfile.files]
    if not channels_last:
        permute_dims = (0, -1) + tuple(range(1, X_trainval.ndim - 1))
        X_trainval = torch.permute(X_trainval, dims=permute_dims)
        X_test = torch.permute(X_test, dims=permute_dims)

    return X_trainval, Y_trainval, X_test, Y_test, folds


def subsample_dataset(x, y, n_sample):
    idx = []
    for i in range(y.shape[1]):
        idx_i = torch.where(y[:, i] == 1)[0]
        idx_i = idx_i[torch.randperm(len(idx_i))[:n_sample]]
        idx.append(idx_i)
    idx = torch.concatenate(idx)
    x = x[idx]
    y = y[idx]

    return x, y

#@title activation functions
# <editor-fold desc="Activation Functions">

def mod2(x):
    return x % 2

activation_dict = {"relu": torch.relu,
                   "mod2": mod2,
                   "square": torch.square,
                   }
activation_shift_dict = {"relu": 0,
                         "mod2": 0.5,
                         "square": 0.5,
                         }
activation_rescale_dict = {"relu": 1,
                           "mod2": 1,
                           "square": 1,
                           }


sgd_activations = list(activation_dict.keys())
forward_activations = list(activation_dict.keys())

# </editor-fold>

# @title forward mlp functions
# <editor-fold desc="Forward mlp training and evaluation functions">

'''Ridge regression function to fit weights'''
def ridge_regression_w(x, y, reg_factor=10, flatten=True, device=device):
    if flatten:
        x = x.reshape((-1, x.shape[-1]))
        y = y.reshape((-1, y.shape[-1]))

    gram_mat = (x.T @ x)
    gram_mat += torch.eye(gram_mat.shape[0], device=device) * reg_factor
    try:
        gram_inv = torch.inverse(gram_mat)
    except:
        gram_inv = torch.eye(gram_mat.shape[0], device=device)
    w_hat = gram_inv @ (x.T @ y)

    if flatten:
        w_hat = torch.unsqueeze(w_hat, dim=0)
    return w_hat


'''
function to fit MLP weight matrix for each layer
'''

def fit_w(x, y, hidden_dim=16,
          flatten=True,
          reg_factor=10.,
          activation="relu",
          return_qu=False,
          device=device,
          training_method="forward_projection",
          ):
    with torch.no_grad():

        if flatten:
            y = y.repeat(repeats=(1, x.shape[1], 1))
            x = x.reshape((-1, x.shape[-1]))
            y = y.reshape((-1, y.shape[-1]))

        q = torch.randn((x.shape[1], hidden_dim), device=device)  # data projection matrix
        u = torch.randn((y.shape[1], hidden_dim), device=device)  # label projection matrix
        y_proj = torch.sign(y @ u)
        match training_method:
            case "forward_projection":
                x_proj = torch.sign(x @ q)
            case "label_projection":
                x_proj = torch.zeros_like(y_proj, device=device)
            case "noisy_label_projection":
                x_proj = torch.sign(torch.randn_like(y_proj, device=device))

        # optional linear transposition
        z = x_proj + y_proj
        z += activation_shift_dict[activation]
        z *= activation_rescale_dict[activation]

        w = ridge_regression_w(x, z,
                               reg_factor=reg_factor,
                               flatten=False,
                               device=device)

        if flatten:
            w = torch.unsqueeze(w, dim=0)

        if return_qu:
            return w, q, u
        else:
            return w, None, None


'''add column of ones to represent intercept'''


def concatenate_ones(x):
    ones_vec = torch.ones_like(x[..., -1, None,])
    x = torch.concatenate([x, ones_vec], dim=-1)
    return x


''' train MLP and return weights and projection matrices'''


def train_forward_mlp(x,
                      y,
                      training_method,
                      activation,
                      hidden_dims=[1000] * 3,
                      reg_factor=10.,
                      return_qu=False,  # returns projection matrices
                      verbose=False,
                      device=device,):
    start_time = time.perf_counter()
    activation_fn = activation_dict[activation]

    w_list = []
    q_list = []
    u_list = []

    x = x.to(device)
    y = y.to(device)

    # fit hidden layers
    for l in range(len(hidden_dims)):

        if verbose:
            print('fitting layer', l)

        # fitting hidden weights
        x = concatenate_ones(x)
        if training_method == "random":
            w = torch.randn((x.shape[-1], hidden_dims[l]), device=device)
            w /= w.norm(dim=-1, keepdim=True)
        if training_method in ["forward_projection", "label_projection", "noisy_label_projection"]:
            w, q, u = fit_w(x, y,
                            hidden_dim=hidden_dims[l],
                            flatten=False,
                            reg_factor=reg_factor,
                            return_qu=return_qu,
                            device=device,
                            activation=activation,
                            training_method=training_method,
                            )
            q_list.append(q)
            u_list.append(u)
        w_list.append(w)

        # forward
        x = activation_fn(x @ w)

    # fitting output layer
    if verbose:
        print('fitting output layer')
    x = concatenate_ones(x)
    w = ridge_regression_w(x, 2 * y - 1, flatten=False, reg_factor=1)
    w_list.append(w)

    end_time = time.perf_counter()
    training_time = end_time - start_time

    return w_list, q_list, u_list, training_time


def evaluate_forward_mlp(x,
                         y,
                         w_list,
                         activation,
                         ):
    activation_fn = activation_dict[activation]

    x = x.to(device)
    y = y.to(device)
    for l in range(len(w_list) - 1):
        x = concatenate_ones(x)
        x = activation_fn(x @ w_list[l])

    x = concatenate_ones(x)
    yhat = x @ w_list[-1]

    test_metrics = compute_metrics(yhat, y)

    return test_metrics


''' evaluate the layer explanation function as a label prediction in each layer'''


def evaluate_explanations_forward_mlp(x,
                                      y,
                                      activation,
                                      w_list,
                                      q_list,
                                      u_list,
                                      ):
    x = x.to(device)
    y = y.to(device)
    activation_fn = activation_dict[activation]

    yhats = []
    for l in range(len(w_list) - 1):
        x = concatenate_ones(x)
        z = x @ w_list[l]
        g_a_q = torch.sign(x @ q_list[l])
        yhat_l = torch.tanh(z - g_a_q) @ torch.linalg.pinv(u_list[l])  # use tanh as a surrogate inverse for sign
        yhats.append(yhat_l)
        x = activation_fn(z)

    x = concatenate_ones(x)
    yhat = x @ w_list[-1]
    yhats.append(yhat)

    test_metrics = [compute_metrics(yhat, y) for yhat in yhats]

    return test_metrics


# </editor-fold>

# @title forward conv1d functions
# <editor-fold desc="Forward conv1d training and evaluation functions (batched)">

'''
function to perform ridge regression over batches of conv1d inputs (channels last)
z refers to z_tilde, the target neural pre-activation potential
'''


def ridge_regression_w_conv1d(x_batches, z_batches, reg_factor=10., device=device):
    x_dim = x_batches[0].shape[-1]
    z_dim = z_batches[1].shape[-1]
    gram_mat = torch.zeros((x_dim, x_dim), device=device)
    xt_z = torch.zeros((x_dim, z_dim), device=device)
    for x_i, z_i in zip(x_batches, z_batches):
        x_i = x_i.to(device)
        z_i = z_i.to(device)
        x_i = x_i.flatten(start_dim=0, end_dim=1)
        z_i = z_i.flatten(start_dim=0, end_dim=1)
        gram_mat += x_i.T @ x_i
        xt_z += x_i.T @ z_i

    # regularise gram matrix
    gram_mat += torch.eye(gram_mat.shape[0], device=device) * reg_factor
    try:
        gram_inv = torch.inverse(gram_mat)
    except:
        print("singular matrix, consider increasing regularisation factor")
        gram_inv = torch.eye(gram_mat.shape[0], device=device)
    w_hat = gram_inv @ xt_z

    return w_hat


'''
function to fit convolutional weights. Channels last
'''


def fit_w_conv1d(x_batches,
                 y_batches,
                 hidden_dim=32,
                 reg_factor=10.,
                 return_qu=False,
                 activation="relu",
                 device=device,
                 training_method="forward_projection",):
    x_channels = x_batches[0].shape[-1]
    y_channels = y_batches[0].shape[-1]

    q = torch.randn((x_channels, hidden_dim), device=device)  # data projection matrix
    u = torch.randn((y_channels, hidden_dim), device=device)  # label projection matrix
    z_batches = []  # batches of targets
    for x_i, y_i in zip(x_batches, y_batches):

        x_i = x_i.to(device)
        y_i = y_i.to(device)

        # project labels
        y_proj = torch.sign(y_i @ u)
        match training_method:
            case "forward_projection":
                x_proj = torch.sign(x_i @ q)
            case "label_projection":
                x_proj = torch.zeros(x_i.shape[:-1] + (u.shape[-1],),
                                     device=device)
            case "noisy_label_projection":
                x_proj = torch.sign(torch.randn(x_i.shape[:-1] + (u.shape[-1],),
                                                device=device))

        # generate target potentials (z)
        z_i = x_proj + y_proj

        # transpose distribution of labels
        if activation_shift_dict[activation] != 0:
            z_i += activation_shift_dict[activation]
        if activation_rescale_dict[activation] != 1:
            z_i *= activation_rescale_dict[activation]
        z_batches.append(z_i.to("cpu"))

    # model target potentials
    w = ridge_regression_w_conv1d(x_batches, z_batches, reg_factor=reg_factor)

    if return_qu:
        return w, q, u
    else:
        return w, None, None


'''
function to fit conv1d neural network (channels last)
convolutional pyramid neural network
'''


def train_forward_conv1d(x,
                         y,
                         training_method,
                         activation='relu',
                         hidden_dim=32,
                         n_blocks=4,
                         kernel_size=3,
                         batch_size=100,
                         return_qu=False,
                         verbose=False,
                         device=device,
                         ):
    with torch.no_grad():

        activation_fn = activation_dict[activation]

        if y.ndim == 2:
            y = torch.unsqueeze(y, dim=1)

        # define hidden layer dimensions for convolutional pyramid
        hidden_dims = [round(hidden_dim * 2 ** (i // 2)) for i in range(n_blocks * 2)]

        start_time = time.perf_counter()
        w_list = []
        q_list = []
        u_list = []

        rand_idx = torch.randperm(len(x))
        x_batches = list(torch.split(x[rand_idx], split_size_or_sections=batch_size))
        y_batches = list(torch.split(y[rand_idx], split_size_or_sections=batch_size))

        # fit hidden layers
        for l in range(len(hidden_dims)):

            if verbose:
                print('fitting layer', l)

            # pooling
            step_size = 2 - ((l + 1) % 2)

            # convolution
            for i in range(len(x_batches)):
                x_i = x_batches[i]
                x_i = x_i.unfold(dimension=1, size=kernel_size, step=step_size).flatten(start_dim=2)
                x_i = concatenate_ones(x_i)
                x_batches[i] = x_i

            # fitting hidden weights
            if training_method == "random":
                w = torch.randn((x_batches[0].shape[-1], hidden_dims[l]), device=device)
                w /= w.norm(dim=-1, keepdim=True)
            if training_method in ["forward_projection", "label_projection", "noisy_label_projection"]:
                w, q, u = fit_w_conv1d(x_batches,
                                       y_batches,
                                       hidden_dim=hidden_dims[l],
                                       activation=activation,
                                       training_method=training_method,
                                       return_qu=return_qu,
                                       )
                q_list.append(q)
                u_list.append(u)
            w_list.append(w)

            # forward pass
            x_batches = [activation_fn(x_i.to(device) @ w).to("cpu") for x_i in x_batches]

        # fitting output layer
        if verbose:
            print('fitting output layer')
        for i in range(len(x_batches)):
            x_batches[i] = concatenate_ones(x_batches[i])
            x_batches[i] = torch.mean(x_batches[i], dim=1, keepdim=True)
            y_batches[i] = 2 * y_batches[i] - 1

        # fit output layer weights
        w = ridge_regression_w_conv1d(x_batches, y_batches, reg_factor=1)  # 1

        w_list.append(w)
        end_time = time.perf_counter()
        training_time = end_time - start_time

        return w_list, q_list, u_list, training_time


def evaluate_forward_conv1d(x,
                            y,
                            w_list,
                            activation,
                            kernel_size=3,
                            batch_size=1000
                            ):
    activation_fn = activation_dict[activation]
    x_batches = torch.split(x, split_size_or_sections=batch_size)

    yhat = []
    for x_i in x_batches:
        x_i = x_i.to(device)

        for l in range(len(w_list) - 1):
            # convolution and pooling
            stride = 2 - ((l + 1) % 2)
            x_i = x_i.unfold(dimension=1, size=kernel_size, step=stride).flatten(start_dim=2)
            x_i = concatenate_ones(x_i)

            # forward
            x_i = activation_fn(x_i @ w_list[l])

        # output
        x_i = concatenate_ones(x_i)
        y = torch.squeeze(y)
        x_i = torch.mean(x_i, dim=1)
        yhat_i = x_i @ w_list[-1]
        yhat.append(yhat_i.to("cpu"))

    yhat = torch.concatenate(yhat)

    metrics = compute_metrics(yhat, y)

    return metrics


# </editor-fold>

#@title forward conv1d experiments
# <editor-fold desc="Forward conv1d experiments">

model_parameters = expand_grid({

    'fold': list(range(5)),
    'training_method': ["forward_projection", "random"],
    'activation': ["relu"],
})
conv1d_datasets = ['human_nontata_promoters']
verbose = True

seed = 0
random.seed(seed)
torch.manual_seed(seed)
np.random.seed(seed)
forward_conv1d_experiments = []
for dataset_i in conv1d_datasets:

    print(dataset_i)
    X_trainval, Y_trainval, X_test, Y_test, folds = load_dataset(dataset_i)

    for model_parameters_i in range(len(model_parameters)):

        print(model_parameters_i)

        training_method = model_parameters.training_method[model_parameters_i]
        fold = model_parameters.fold[model_parameters_i]
        activation = model_parameters.activation[model_parameters_i]
        hidden_dim = 32
        n_blocks = 4

        # train 1d conv
        train_folds = folds != fold
        val_folds = torch.logical_not(train_folds)
        X_train, X_val = X_trainval[train_folds], X_trainval[val_folds]
        Y_train, Y_val = Y_trainval[train_folds], Y_trainval[val_folds]

        w_list, _, _, training_time = train_forward_conv1d(x=X_train,
                                                           y=Y_train,
                                                           training_method=training_method,
                                                           hidden_dim=hidden_dim,
                                                           activation=activation,
                                                           n_blocks=n_blocks,
                                                           device=device,
                                                           batch_size=1000
                                                           )
        train_metrics = evaluate_forward_conv1d(x=X_train,
                                                y=Y_train,
                                                w_list=w_list,
                                                activation=activation)
        val_metrics = evaluate_forward_conv1d(x=X_val,
                                              y=Y_val,
                                              w_list=w_list,
                                              activation=activation)
        test_metrics = evaluate_forward_conv1d(x=X_test,
                                               y=Y_test,
                                               w_list=w_list,
                                               activation=activation)
        if verbose:
            print(training_method)
            print(train_metrics)
            print(val_metrics)
            print(test_metrics)

        out = {
            'dataset': dataset_i,
            'training_method': training_method,
            'fold': fold,
            'activation': activation,
            'hidden_dim': hidden_dim,
            'n_blocks': n_blocks,
            'train_auc': train_metrics[0].item(),
            'train_acc': train_metrics[1].item(),
            'train_prec': train_metrics[2].item(),
            'train_recall': train_metrics[3].item(),
            'train_f1': train_metrics[4].item(),
            'val_auc': val_metrics[0].item(),
            'val_acc': val_metrics[1].item(),
            'val_prec': val_metrics[2].item(),
            'val_recall': val_metrics[3].item(),
            'val_f1': val_metrics[4].item(),
            'test_auc': test_metrics[0].item(),
            'test_acc': test_metrics[1].item(),
            'test_prec': test_metrics[2].item(),
            'test_recall': test_metrics[3].item(),
            'test_f1': test_metrics[4].item(),
            'training_time': training_time,
        }
        forward_conv1d_experiments.append(out)
forward_conv1d_experiments = pd.DataFrame(forward_conv1d_experiments)
output_file = os.path.join(output_dir, "forward_conv1d_experiments.csv")
forward_conv1d_experiments.to_csv(path_or_buf=output_file)


# </editor-fold>

#@title forward conv2d functions
# <editor-fold desc="Forward conv2d training and evaluation functions (batched)">

'''
weights over conv2d data batches. Channels last.
z refers to the target potentials
'''
def ridge_regression_w_conv2d(x_batches, z_batches, reg_factor=10., device=device):
    x_dim = x_batches[0].shape[-1] # data
    z_dim = z_batches[1].shape[-1] # targets

    #accumulate data gram matrix and cross product of data and targets
    gram_mat = torch.zeros((x_dim, x_dim), device=device)
    xt_z = torch.zeros((x_dim, z_dim), device=device)
    for x_i, z_i in zip(x_batches, z_batches):
        x_i = x_i.to(device)
        z_i = z_i.to(device)
        x_i = x_i.flatten(start_dim=0, end_dim=2)
        z_i = z_i.flatten(start_dim=0, end_dim=2)
        xt_z += x_i.T @ z_i
        gram_mat += x_i.T @ x_i

    #regularise
    gram_mat += torch.eye(gram_mat.shape[0], device=device) * reg_factor

    #invert gram matrix
    try:
        gram_inv = torch.inverse(gram_mat)
    except:
        print("singular gram matrix, consider increasing regularisation factor")
        gram_inv = torch.eye(gram_mat.shape[0], device=device)

    #compute weight matrix
    w_hat = gram_inv @ xt_z

    return w_hat

'''
function to fit 2d convolutional layer weights over data batches
channels last

'''

def fit_w_conv2d(x_batches,
                 y_batches,
                 hidden_dim=16,
                 reg_factor=0.01,
                 return_qu=False,
                 activation="relu",
                 device=device,
                 training_method="forward_projection"):
    x_channels = x_batches[0].shape[-1]
    y_channels = y_batches[0].shape[-1]

    q = torch.randn((x_channels, hidden_dim), device=device) # data projection matrix
    u = torch.randn((y_channels, hidden_dim), device=device) # label projection matx
    z_batches = [] # targets
    for x_i, y_i in zip(x_batches, y_batches):

        x_i = x_i.to(device)
        y_i = y_i.to(device)

        #generate target values (z)
        y_proj = torch.sign(y_i @ u)
        match training_method:
            case "forward_projection":
                x_proj = torch.sign(x_i @ q)
            case "label_projection":
                x_proj = torch.zeros(x_i.shape[:-1] + (u.shape[-1],),
                                     device=device)
            case "noisy_label_projection":
                x_proj = torch.sign(torch.randn(x_i.shape[:-1] + (u.shape[-1],),
                                                device=device))

        z_i = x_proj + y_proj

        # transpose target distribution
        if activation_shift_dict[activation] != 0:
            z_i += activation_shift_dict[activation]
        if activation_rescale_dict[activation] != 1:
            z_i *= activation_rescale_dict[activation]
        z_batches.append(z_i.to("cpu"))

    #fit weight
    w = ridge_regression_w_conv2d(x_batches, z_batches, reg_factor=reg_factor)

    if return_qu:
        return w, q, u
    else:
        return w, None, None


def train_forward_conv2d(x,
                         y,
                         training_method,
                         activation='relu',
                         hidden_dim=16,
                         n_blocks=3,
                         kernel_size=3,
                         batch_size=100,
                         reg_factor=0.01,
                         return_qu=False,
                         verbose=False,
                         device=device,
                         ):
    activation_fn = activation_dict[activation]

    if y.ndim == 2:
        y = y[:, None, None, :]

    hidden_dims = [round(hidden_dim * 2 ** (i // 2)) for i in range(n_blocks * 2)]

    start_time = time.perf_counter()
    w_list = [] # layer weight matrices
    q_list = [] # data projection matrices
    u_list = [] # label projeciton matrices

    rand_idx = torch.randperm(len(x))
    x_batches = list(torch.split(x[rand_idx], split_size_or_sections=batch_size))
    y_batches = list(torch.split(y[rand_idx], split_size_or_sections=batch_size))

    # fit hidden layers
    for l in range(len(hidden_dims)):

        if verbose:
            print('fitting layer', l)

        # pooling
        stride = 2 - ((l + 1) % 2)

        # convolution
        for i in range(len(x_batches)):
            x_i = x_batches[i]
            x_i = x_i.unfold(dimension=1, size=kernel_size, step=stride)  #
            x_i = x_i.unfold(dimension=2, size=kernel_size, step=stride)  #
            x_i = x_i.flatten(start_dim=3)
            x_i = concatenate_ones(x_i)
            x_batches[i] = x_i

        # fitting hidden weights
        if training_method == "random":
            w = torch.randn((x_batches[0].shape[-1], hidden_dims[l])).to(device)
            w /= w.norm(dim=-1, keepdim=True)
        if training_method in ["forward_projection", "label_projection", "noisy_label_projection"]:
            w, q, u = fit_w_conv2d(x_batches,
                                   y_batches,
                                   hidden_dim=hidden_dims[l],
                                   return_qu=return_qu,
                                   activation=activation,
                                   training_method=training_method,
                                   reg_factor=reg_factor)
            q_list.append(q)
            u_list.append(u)
        w_list.append(w)

        # forward
        x_batches = [activation_fn(x_i.to(device) @ w).to("cpu") for x_i in x_batches]

    # fitting output layer
    if verbose:
        print('fitting output layer')
    for i in range(len(x_batches)):
        x_batches[i] = concatenate_ones(x_batches[i])
        x_batches[i] = torch.mean(x_batches[i], dim=(1, 2), keepdim=True)
        y_batches[i] = 2 * y_batches[i] - 1

    # fit weight
    w = ridge_regression_w_conv2d(x_batches, y_batches, reg_factor=1)

    w_list.append(w)
    end_time = time.perf_counter()
    training_time = end_time - start_time

    return w_list, q_list, u_list, training_time


def evaluate_forward_conv2d(x,
                            y,
                            w_list,
                            activation,
                            kernel_size=3,
                            batch_size=1000
                            ):
    activation_fn = activation_dict[activation]
    x_batches = torch.split(x, split_size_or_sections=batch_size)

    yhat = []
    for x_i in x_batches:
        x_i = x_i.to(device)

        for l in range(len(w_list) - 1):

            # convolution and pooling
            stride = 2 - ((l + 1) % 2)
            x_i = x_i.unfold(dimension=1, size=kernel_size, step=stride)  #
            x_i = x_i.unfold(dimension=2, size=kernel_size, step=stride)  #
            x_i = x_i.flatten(start_dim=3)
            x_i = concatenate_ones(x_i)

            # forward
            x_i = activation_fn(x_i @ w_list[l])

        # output
        x_i = concatenate_ones(x_i)
        y = torch.squeeze(y)

        # extract global average as prediction
        x_i = torch.mean(x_i, dim=(1, 2))
        yhat_i = x_i @ w_list[-1]
        yhat.append(yhat_i.to("cpu"))

    yhat = torch.concatenate(yhat)

    metrics = compute_metrics(yhat, y)

    return metrics



# </editor-fold>

#@title sgd training and evaluation functions

def train_sgd(model,
              x,
              y,
              loss_fn,
              batch_size=10, ):
    torch.cuda.empty_cache()
    model.train()

    # training
    rand_idx = torch.randperm(len(x))
    x_batches = torch.split(x[rand_idx], split_size_or_sections=batch_size)
    y_batches = torch.split(y[rand_idx], split_size_or_sections=batch_size)
    train_loss = 0
    if model.training_method == "backprop":
        for x_i, y_i in zip(x_batches, y_batches):
            x_i = x_i.to(device)
            y_i = y_i.to(device)
            yhat_i = model(x_i)
            loss = loss_fn(yhat_i, y_i)
            model.opt.zero_grad()
            loss.backward()
            model.opt.step()
            train_loss += loss
    if model.training_method == "local_supervision":
        for x_i, y_i in zip(x_batches, y_batches):
            x_i = x_i.to(device)
            y_i = y_i.to(device)
            yhats = model.forward_ls(x_i)
            for l in range(len(yhats)):
                loss_l = loss_fn(yhats[l], y_i)
                model.opts[l].zero_grad()
                loss_l.backward(inputs=tuple(model.layers[l].parameters()))
                model.opts[l].step()
            train_loss += loss_l / len(yhats)
    if model.training_method == "forward_forward":
        for x_i, y_i in zip(x_batches, y_batches):
            x_i = x_i.to(device)
            y_i = y_i.to(device)
            y_neg_i = torch.argmax(torch.rand_like(y_i) - y_i, dim=1).to(device)
            y_neg_i = torch.eye(y_i.shape[1]).to(device)[y_neg_i]
            while y_i.ndim < x_i.ndim:
                y_i = torch.unsqueeze(y_i, dim=-1)
                y_neg_i = torch.unsqueeze(y_neg_i, dim=-1)
            y_i = y_i.repeat(repeats=(1, 1) + x_i.shape[2:])
            y_neg_i = y_neg_i.repeat(repeats=(1, 1) + x_i.shape[2:])
            xy_pos = torch.concatenate([x_i, y_i], dim=1)
            xy_neg = torch.concatenate([x_i, y_neg_i], dim=1)
            g_pos = model.forward_ff(xy_pos)
            g_neg = model.forward_ff(xy_neg)
            for l in range(len(g_pos)):
                loss_l = torch.log(1 + torch.exp(torch.concatenate([2 - g_pos[l], g_neg[l] - 2]))).mean()
                model.opts[l].zero_grad()
                loss_l.backward(inputs=tuple(model.layers[l].parameters()))
                model.opts[l].step()
                train_loss += loss_l / len(x_batches)
    if model.training_method == "predictive_coding":
        for x_i, y_i in zip(x_batches, y_batches):
            x_i = x_i.to(device)
            y_i = y_i.to(device)
            local_losses, yhat = model.forward_pc(x_i)
            for l in range(len(model.layers) - 1):
                model.opts[l].zero_grad()
                model.back_opts[l].zero_grad()
                local_losses[l].backward()
                model.opts[l].step()
                model.back_opts[l].step()
            loss_l = loss_fn(yhat, y_i)
            model.opts[-1].zero_grad()
            loss_l.backward(inputs=tuple(model.layers[-1].parameters()))
            model.opts[-1].step()
            train_loss += loss_l / len(x_batches)
    if model.training_method == "difference_target_propagation":
        for x_i, y_i in zip(x_batches, y_batches):
            x_i = x_i.to(device)
            y_i = y_i.to(device)
            local_losses, yhat = model.forward_pc(x_i)
            for l in range(len(model.layers) - 1):
                model.opts[l].zero_grad()
                model.back_opts[l+1].zero_grad()
                local_losses[l].backward()
                model.opts[l].step()
                model.back_opts[l+1].step()
            loss_l = loss_fn(yhat, y_i)
            model.opts[-1].zero_grad()
            loss_l.backward(inputs=tuple(model.layers[-1].parameters()))
            model.opts[-1].step()
            train_loss += loss_l / len(x_batches)


    if model.training_method == "direct_random_target_projection":
        for x_i, y_i in zip(x_batches, y_batches):
            x_i = x_i.to(device)
            y_i = y_i.to(device)
            x = x_i.detach()
            for l in range(len(model.layers) - 1):
                x = x.detach()
                x = model.layers[l](x)
                x = model.activation_fn(x)
                target = model.forward_proj[l](y_i)
                target = model.activation_fn(target)
                loss_l = torch.mean((x - target) ** 2)
                model.opts[l].zero_grad()
                loss_l.backward(inputs=tuple(model.layers[l].parameters()))
                model.opts[l].step()
            x = x.detach()
            yhat = model.layers[-1](x)
            loss_l = loss_fn(yhat, y_i)
            model.opts[-1].zero_grad()
            loss_l.backward(inputs=tuple(model.layers[-1].parameters()))
            model.opts[-1].step()
            train_loss += loss_l / len(x_batches)

    return train_loss


def validate_sgd(model,
                 x,
                 y,
                 loss_fn,
                 batch_size=25, ):
    torch.cuda.empty_cache()
    model.eval()
    with torch.no_grad():
        val_loss = 0
        x_batches = torch.split(x, split_size_or_sections=batch_size)
        y_batches = torch.split(y, split_size_or_sections=batch_size)
        if model.training_method != "forward_forward":
            for x_i, y_i in zip(x_batches, y_batches):
                x_i = x_i.to(device)
                y_i = y_i.to(device)
                yhat_i = model(x_i)
                loss_i = loss_fn(yhat_i, y_i)
                val_loss += loss_i
        else:
            for x_i, y_i in zip(x_batches, y_batches):
                x_i = x_i.to(device)
                y_i = y_i.to(device)
                y_neg_i = torch.argmax(torch.rand_like(y_i) - y_i, dim=1).to(device)
                y_neg_i = torch.eye(y_i.shape[1]).to(device)[y_neg_i]
                while y_i.ndim < x_i.ndim:
                    y_i = torch.unsqueeze(y_i, dim=-1)
                    y_neg_i = torch.unsqueeze(y_neg_i, dim=-1)
                y_i = y_i.repeat(repeats=(1, 1) + x_i.shape[2:])
                y_neg_i = y_neg_i.repeat(repeats=(1, 1) + x_i.shape[2:])
                xy_pos = torch.concatenate([x_i, y_i], dim=1)
                xy_neg = torch.concatenate([x_i, y_neg_i], dim=1)
                g_pos = model.forward_ff(xy_pos)
                g_neg = model.forward_ff(xy_neg)
                for g_pos_l, g_neg_l in zip(g_pos, g_neg):
                    loss_l = torch.log(1 + torch.exp(torch.concatenate([2 - g_pos_l, g_neg_l - 2]))).mean()
                    val_loss += loss_l / len(g_pos)
        val_loss /= len(x_batches)
        return val_loss


def evaluate_sgd(model,
                 x,
                 y,
                 batch_size=25,
                 ):
    model.eval()
    with torch.no_grad():
        torch.cuda.empty_cache()
        x_batches = torch.split(x, split_size_or_sections=batch_size)
        if model.training_method != "forward_forward":
            yhat = [model(x_i.to(device)) for x_i in x_batches]
        else:
            yhat = []
            n_classes = y.shape[1]
            for x_i in x_batches:
                x_i = x_i.to(device)
                y_candidates = torch.eye(n_classes).unsqueeze(1).repeat(1, len(x_i), 1).to(device)
                while y_candidates.ndim < (x_i.ndim + 1):
                    y_candidates = torch.unsqueeze(y_candidates, dim=-1)
                y_candidates = y_candidates.repeat(repeats=(1, 1, 1) + x_i.shape[2:])
                yhat_i = torch.zeros((len(x_i), n_classes))
                for j in range(n_classes):
                    xy_ij = torch.concatenate([x_i, y_candidates[j]], dim=1)
                    goodness_i = model.forward_ff(xy_ij)
                    yhat_i[:, j] = torch.mean(torch.stack(goodness_i), dim=0)
                yhat.append(yhat_i)

        yhat = torch.concatenate(yhat)
        metrics = compute_metrics(yhat, y)
        return metrics

#@title sgd mlp functions
# <editor-fold desc="SGD mlp training and evaluation functions">


sgd_training_methods = [
    "backprop",
    "local_supervision",
    "forward_forward",
    "predictive_coding",
    "difference_target_propagation",
]
selected_sgd_training_methods = [
    "backprop",
    "local_supervision",]

class mlpModel(nn.Module):
    def __init__(self,
                 training_method,
                 activation_fn,
                 in_features=None,
                 hidden_dims=None,
                 num_classes=None,
                 ):
        super(mlpModel, self).__init__()
        self.activation_fn = activation_fn
        self.in_features = [in_features] + hidden_dims
        if training_method == "forward_forward":
            self.in_features[0] += num_classes
        self.out_features = hidden_dims + [num_classes]
        self.layers = nn.ModuleList(
            [nn.Linear(in_l, out_l) for in_l, out_l in zip(self.in_features, self.out_features)])
        self.training_method = training_method
        self.detach_output = training_method != "backprop"
        if training_method == "backprop":
            self.opt = torch.optim.Adam(self.layers.parameters(), )
        if training_method == "local_supervision":
            self.opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.layers]
        if training_method == "forward_forward":
            self.opts = [torch.optim.Adam(layer_l.parameters(),
                                          lr=0.03) for layer_l in self.layers]
        if training_method == "local_supervision":
            self.ls_layers = nn.ModuleList([nn.Linear(out_l, num_classes) for out_l in hidden_dims])
        if training_method == "predictive_coding":
            self.back_layers = nn.ModuleList(
                [nn.Linear(out_l, in_l) for in_l, out_l in zip(self.in_features[:-1], self.out_features[:-1])])
            self.opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.layers]
            self.back_opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.back_layers]
        if training_method == "direct_random_target_projection":
            self.opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.layers]
            self.forward_proj = nn.ModuleList([nn.Linear(num_classes, out_l) for out_l in hidden_dims])
        if training_method == "difference_target_propagation":
            self.opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.layers]
            self.back_layers = nn.ModuleList(
                [nn.Linear(in_l, out_l) for in_l, out_l in zip(self.out_features, self.in_features)])
            self.back_opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.back_layers]

    def forward(self, x):
        for layer_l in self.layers[:-1]:
            x = layer_l(x)
            x = self.activation_fn(x)
        if self.detach_output:
            x = torch.detach(x)
        x = self.layers[-1](x)
        return x

    def forward_ls(self, x):
        yhats = []
        for layer_l, ls_layer_l in zip(self.layers[:-1], self.ls_layers):
            x = layer_l(x)
            x = self.activation_fn(x)
            yhat_l = ls_layer_l(x)
            yhats.append(yhat_l)
            x = torch.detach(x)
        yhats.append(self.layers[-1](x))
        return yhats

    def forward_ff(self, xy):
        goodnesses = []
        for layer_l in self.layers[:-1]:
            xy = xy / (xy.norm(dim=1, keepdim=True) + 0.001)
            xy = layer_l(xy)
            xy = self.activation_fn(xy)
            goodness_l = xy.square().mean(dim=1)
            goodnesses.append(goodness_l)
            xy = torch.detach(xy)
        return goodnesses

    def forward_pc(self, x):
        local_losses = []
        for l in range(len(self.layers) - 1):
            x_out = self.layers[l](x)
            x_out = self.activation_fn(x_out)
            xhat = self.back_layers[l](x_out)
            xhat = self.activation_fn(xhat)
            loss_l = torch.mean((xhat - x) ** 2)
            local_losses.append(loss_l)
            x = x_out.detach()
        yhat = self.layers[-1](x)
        return local_losses, yhat

    def forward_dtp(self, x):

        h_list = []
        for l in range(len(self.layers) - 1):
            x = self.layers[l](x)
            x = self.activation_fn(x)
            h_list.append(x)
            x = x.detach()
        yhat = self.layers[-1](x)
        h_list.append(yhat)

        hhat_list = [None] * (len(h_list))
        hhat_list[-1] = h_list[-1].detach()
        for l in list(range(1, len(h_list))):
            h_curr = h_list[-l].detach()
            h_prev = h_list[-(l+1)].detach()
            hhat_curr = hhat_list[-l].detach()
            ghhat_curr = self.back_layers[-l](hhat_curr)
            ghhat_curr = self.activation_fn(ghhat_curr)
            gh_curr = self.back_layers[-l](h_curr)
            gh_curr = self.activation_fn(gh_curr)
            hhat_prev = h_prev + ghhat_curr - gh_curr
            hhat_list[-(l+1)] = hhat_prev
        local_losses = []
        for l in range(len(h_list)-1):
            loss_l = torch.mean((hhat_list[l] - h_list[l]) ** 2)
            local_losses.append(loss_l)

        return local_losses, yhat




def train_sgd_mlp(X_train,
                  Y_train,
                  X_val,
                  Y_val,
                  activation,
                  training_method,
                  hidden_dims=[1000] * 3,
                  patience=5,
                  max_epochs=100,
                  batch_size=50,
                  verbose=False,
                  loss_fn=torch.nn.CrossEntropyLoss()):
    torch.cuda.empty_cache()
    start_time = time.perf_counter()
    activation_fn = activation_dict[activation]

    in_features = X_train.shape[1]
    model = mlpModel(
        training_method=training_method,
        in_features=in_features,
        hidden_dims=hidden_dims,
        num_classes=Y_train.shape[1],
        activation_fn=activation_fn,
    ).to(device)
    train_loss = []
    val_loss = []
    best_val_loss = torch.inf
    patience_counter = 0
    for epoch_i in range(max_epochs):
        train_loss_i = train_sgd(model=model,
                                 x=X_train,
                                 y=Y_train,
                                 loss_fn=loss_fn,
                                 batch_size=batch_size)
        train_loss.append(train_loss_i.item())
        val_loss_i = validate_sgd(model=model,
                                  x=X_val,
                                  y=Y_val,
                                  loss_fn=loss_fn,
                                  batch_size=batch_size)
        val_loss.append(val_loss_i.item())
        if verbose:
            print(val_loss_i)
        if val_loss_i < best_val_loss:
            best_val_loss = val_loss_i
            patience_counter = 0
        else:
            patience_counter += 1
        if patience_counter == (patience - 1):
            break

    end_time = time.perf_counter()
    training_time = end_time - start_time
    training_epochs = epoch_i

    return model, training_time, training_epochs

# </editor-fold>

#@title sgd conv1d functions
# <editor-fold desc="SGD conv1d training and evaluation functions">

class conv1dModel(nn.Module):
    def __init__(self,
                 training_method,
                 activation_fn,
                 in_features=None,
                 hidden_dims=None,
                 num_classes=None,
                 kernel_size=3,
                 ):
        super(conv1dModel, self).__init__()
        self.activation_fn = activation_fn
        self.in_features = [in_features] + hidden_dims
        if training_method == "forward_forward":
            self.in_features[0] += num_classes
        self.out_features = hidden_dims + [num_classes]
        self.layers = []
        for in_l, out_l in zip(self.in_features[:-1], self.out_features[:-1]):
            self.layers.append(nn.Conv1d(in_l, out_l, kernel_size))
        self.layers.append(nn.Linear(self.in_features[-1], self.out_features[-1]))
        self.layers = nn.ModuleList(self.layers)
        self.batch_norms = nn.ModuleList([nn.BatchNorm1d(out_l) for out_l in hidden_dims[1::2]])
        self.training_method = training_method
        if training_method == "backprop":
            self.opt = torch.optim.Adam(self.layers.parameters())
        if training_method == "local_supervision":
            self.ls_layers = nn.ModuleList(
                [nn.Linear(out_l, num_classes) for out_l in hidden_dims])
            self.opts = [torch.optim.Adam(layer_l.parameters()) for layer_l in self.layers]
        if training_method == "forward_forward":
            self.opts = [torch.optim.Adam(layer_l.parameters(),
                                          lr=0.03) for layer_l in self.layers]
        if training_method == "predictive_coding":
            self.back_layers = nn.ModuleList([])
            for out_l, in_l in zip(self.in_features[:-1], self.out_features[:-1]):
                self.back_layers.append(nn.ConvTranspose1d(in_l, out_l, kernel_size))
            self.opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.layers]
            self.back_opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.back_layers]
        if training_method == "difference_target_propagation":
            self.back_layers = nn.ModuleList([])
            for out_l, in_l in zip(self.in_features[:-1], self.out_features[:-1]):
                self.back_layers.append(nn.ConvTranspose1d(in_l, out_l, kernel_size))
            self.back_layers.append(nn.Linear(self.out_features[-1], self.in_features[-1]))
            self.opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.layers]
            self.back_opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.back_layers]


    def forward(self, x):
        for l in range(len(self.layers) - 1):
            x = self.layers[l](x)
            x = self.activation_fn(x)
            if l % 2:
                x = x[..., ::2]
                if l < len(self.batch_norms):
                    x = self.batch_norms[l // 2](x)
        x = torch.mean(x, dim=2)
        yhat = self.layers[-1](x)
        return yhat

    def forward_ls(self, x):
        yhats = []
        for l in range(len(self.layers) - 1):
            x = self.layers[l](x)
            x = self.activation_fn(x)
            x_mu = torch.mean(x, dim=2)
            yhat_l = self.ls_layers[l](x_mu)
            yhats.append(yhat_l)
            if l % 2:
                x = x[..., ::2]
                if l < len(self.batch_norms):
                    x = self.batch_norms[l // 2](x)
            x = torch.detach(x)
        x = torch.mean(x, dim=2)
        yhat = self.layers[-1](x)
        yhats.append(yhat)
        return yhats

    def forward_ff(self, xy):
        goodnesses = []
        for l in range(len(self.layers) - 1):
            xy = xy / (xy.norm(dim=1, keepdim=True) + 0.001)
            xy = self.layers[l](xy)
            xy = self.activation_fn(xy)
            goodness_l = xy.square().mean(dim=(1, 2))
            goodnesses.append(goodness_l)
            if l % 2:
                xy = xy[..., ::2]
                if l < len(self.batch_norms):
                    xy = self.batch_norms[l // 2](xy)
            xy = torch.detach(xy)
        return goodnesses

    def forward_pc(self, x):
        local_losses = []
        for l in range(len(self.layers) - 1):
            x_out = self.layers[l](x)
            x_out = self.activation_fn(x_out)
            xhat = self.back_layers[l](x_out)
            xhat = self.activation_fn(xhat)
            loss_l = torch.mean((xhat - x) ** 2)
            local_losses.append(loss_l)
            x = x_out.detach()
            if l % 2:
                x = x[..., ::2]
                if l < len(self.batch_norms):
                    x = self.batch_norms[l // 2](x)
        x = torch.mean(x, dim=2)
        yhat = self.layers[-1](x)
        return local_losses, yhat

    def forward_dtp(self, x):

        h_list = []
        for l in range(len(self.layers) - 1):
            x = self.layers[l](x)
            x = self.activation_fn(x)
            h_list.append(x)
            x = x.detach()
            if l % 2:
                x = x[..., ::2]
                if l < len(self.batch_norms):
                    x = self.batch_norms[l // 2](x)
        out_shape = x.shape[2]
        x = x.mean(dim=2)
        yhat = self.layers[-1](x)
        h_list.append(yhat)

        hhat_list = [None] * (len(h_list))
        hhat_list[-1] = h_list[-1].detach()
        for l in list(range(1, len(h_list))):
            h_curr = h_list[-l].detach()
            h_prev = h_list[-(l + 1)].detach()
            hhat_curr = hhat_list[-l].detach()
            ghhat_curr = self.back_layers[-l](hhat_curr)
            ghhat_curr = self.activation_fn(ghhat_curr)
            gh_curr = self.back_layers[-l](h_curr)
            gh_curr = self.activation_fn(gh_curr)
            if l == 1:
                gh_curr = gh_curr[..., None].repeat((1, 1, out_shape))
                ghhat_curr = ghhat_curr[..., None].repeat((1, 1, out_shape))
            if (l % 2) == 0:
                gh_curr = gh_curr.repeat_interleave(repeats=2, dim=2)
                ghhat_curr = ghhat_curr.repeat_interleave(repeats=2, dim=2)
            hhat_prev = h_prev + ghhat_curr - gh_curr
            hhat_list[-(l + 1)] = hhat_prev
        local_losses = []
        for l in range(len(h_list) - 1):
            loss_l = torch.mean((hhat_list[l] - h_list[l]) ** 2)
            local_losses.append(loss_l)

        return local_losses, yhat



def train_sgd_conv1d(X_train,
                     Y_train,
                     X_val,
                     Y_val,
                     hidden_dims,
                     activation,
                     training_method,
                     kernel_size=3,
                     patience=5,
                     max_epochs=50,
                     batch_size=25,
                     verbose=False,
                     loss_fn=torch.nn.CrossEntropyLoss()):
    torch.cuda.empty_cache()
    start_time = time.perf_counter()

    activation_fn = activation_dict[activation]
    model = conv1dModel(
        in_features=X_train.shape[1],
        hidden_dims=hidden_dims,
        num_classes=Y_train.shape[1],
        activation_fn=activation_fn,
        kernel_size=kernel_size,
        training_method=training_method
    ).to(device)
    train_loss = []
    val_loss = []
    best_val_loss = torch.inf
    patience_counter = 0
    for epoch_i in range(max_epochs):
        train_loss_i = train_sgd(model=model,
                                 x=X_train,
                                 y=Y_train,
                                 loss_fn=loss_fn,
                                 batch_size=batch_size
                                 )
        train_loss.append(train_loss_i.item())
        val_loss_i = validate_sgd(model=model,
                                  x=X_val,
                                  y=Y_val,
                                  loss_fn=loss_fn,
                                  batch_size=batch_size)
        val_loss.append(val_loss_i.item())
        if verbose:
            print(val_loss_i)
        if val_loss_i < best_val_loss:
            best_val_loss = val_loss_i
            patience_counter = 0
        else:
            patience_counter += 1
        if patience_counter == (patience - 1):
            break

    end_time = time.perf_counter()
    training_time = end_time - start_time
    training_epochs = epoch_i

    model = conv1dModel(
        in_features=X_train.shape[1],
        hidden_dims=hidden_dims,
        num_classes=Y_train.shape[1],
        activation_fn=activation_fn,
        kernel_size=kernel_size,
        training_method=training_method
    ).to(device)
    print("final training")
    X_trainval = torch.concatenate([X_train, X_val])
    Y_trainval = torch.concatenate([Y_train, Y_val])
    for _ in range(epoch_i):
        _ = train_sgd(model=model,
                      x=X_trainval,
                      y=Y_trainval,
                      loss_fn=loss_fn,
                      batch_size=batch_size)

    return model, training_time, training_epochs


# </editor-fold>

#@title sgd conv1d experiments

# <editor-fold desc="SGD conv1d experiments">

sgd_training_methods = [
    "backprop",
    "local_supervision",
    "forward_forward",
    "predictive_coding",
    "difference_target_propagation",
]

model_parameters = expand_grid({
    'fold': torch.arange(5),
    'activation': ["relu"],
    'training_method': ["backprop","local_supervision"],
})

conv1d_datasets = ['human_nontata_promoters']

verbose = False

seed = 0
random.seed(seed)
torch.manual_seed(seed)
np.random.seed(seed)
sgd_conv1d_experiments = []
for dataset_i in conv1d_datasets:

    print(dataset_i)

    X_trainval, Y_trainval, X_test, Y_test, folds = load_dataset(dataset_i, channels_last=False)

    for model_parameters_i in range(len(model_parameters)):
        print(model_parameters_i)

        fold = model_parameters.fold[model_parameters_i]
        activation = model_parameters.activation[model_parameters_i]
        training_method = model_parameters.training_method[model_parameters_i]
        hidden_dim = 32
        n_blocks = 4
        hidden_dims = [hidden_dim * 2 ** (i // 2) for i in range(n_blocks * 2)]

        train_folds = folds != fold
        val_folds = torch.logical_not(train_folds)
        X_train, X_val = X_trainval[train_folds], X_trainval[val_folds]
        Y_train, Y_val = Y_trainval[train_folds], Y_trainval[val_folds]

        model, training_time, training_epochs = train_sgd_conv1d(
            X_train=X_train,
            Y_train=Y_train,
            X_val=X_val,
            Y_val=Y_val,
            hidden_dims=hidden_dims,
            activation=activation,
            training_method=training_method,
            verbose=True)

        train_metrics = evaluate_sgd(model=model,
                                    x=X_train,
                                    y=Y_train,
                                    )

        val_metrics = evaluate_sgd(model=model,
                                  x=X_val,
                                  y=Y_val,
                                  )

        test_metrics = evaluate_sgd(model=model,
                                    x=X_test,
                                    y=Y_test,
                                    )
        if verbose:
            print(training_method, dataset_i, activation, sep="\n")
            print(train_metrics)
            print(val_metrics)
            print(test_metrics)

        out = {
            'dataset': dataset_i,
            'training_method': training_method,
            'fold': fold,
            'activation': activation,
            'hidden_dim': hidden_dim,
            'n_blocks': n_blocks,
            'train_auc': train_metrics[0].item(),
            'train_acc': train_metrics[1].item(),
            'train_prec': train_metrics[2].item(),
            'train_recall': train_metrics[3].item(),
            'train_f1': train_metrics[4].item(),
            'val_auc': val_metrics[0].item(),
            'val_acc': val_metrics[1].item(),
            'val_prec': val_metrics[2].item(),
            'val_recall': val_metrics[3].item(),
            'val_f1': val_metrics[4].item(),
            'test_auc': test_metrics[0].item(),
            'test_acc': test_metrics[1].item(),
            'test_prec': test_metrics[2].item(),
            'test_recall': test_metrics[3].item(),
            'test_f1': test_metrics[4].item(),
            'training_time': training_time,
          'training_epochs': training_epochs,
        }
        sgd_conv1d_experiments.append(out)

sgd_conv1d_experiments = pd.DataFrame(sgd_conv1d_experiments)
output_file = os.path.join(output_dir, "sgd_conv1d_experiments.csv")
sgd_conv1d_experiments.to_csv(path_or_buf=output_file)


    # </editor-fold>

#@title sgd conv2d functions
# <editor-fold desc="SGD conv2d training and evaluation functions">

class conv2dModel(nn.Module):
    def __init__(self,
                 training_method,
                 activation_fn,
                 in_features=None,
                 hidden_dims=None,
                 num_classes=None,
                 kernel_size=3,
                 pad=0,
                 lr=0.001
                 ):
        super().__init__()
        self.activation_fn = activation_fn
        self.in_features = [in_features] + hidden_dims
        self.pad = pad
        if training_method == "forward_forward":
            self.in_features[0] += num_classes
        self.out_features = hidden_dims + [num_classes]
        self.layers = []
        for in_l, out_l in zip(self.in_features[:-1], self.out_features[:-1]):
            self.layers.append(nn.Conv2d(in_l, out_l, kernel_size, padding=self.pad))
        self.layers.append(nn.Linear(self.in_features[-1], self.out_features[-1]))
        self.layers = nn.ModuleList(self.layers)
        self.batch_norms = nn.ModuleList([nn.BatchNorm2d(out_l) for out_l in hidden_dims[1::2]])
        self.training_method = training_method
        if training_method == "backprop":
            self.opt = torch.optim.Adam(self.layers.parameters(), lr=lr)
        if training_method == "local_supervision":
            self.ls_layers = nn.ModuleList(
                [nn.Linear(out_l, num_classes) for out_l in hidden_dims])
            self.opts = [torch.optim.Adam(layer_l.parameters(), lr=lr) for layer_l in self.layers]
        if training_method == "forward_forward":
            self.opts = [torch.optim.Adam(layer_l.parameters(),
                                          lr=lr) for layer_l in self.layers]
        if training_method == "direct_random_target_projection":
            self.ls_layers = nn.ModuleList(
                [nn.Linear(num_classes, out_l) for out_l in hidden_dims])
        if training_method == "predictive_coding":
            self.back_layers = nn.ModuleList([])
            for out_l, in_l in zip(self.in_features[:-1], self.out_features[:-1]):
                self.back_layers.append(nn.ConvTranspose2d(in_l, out_l, kernel_size, padding=self.pad))
            self.opts = [torch.optim.Adam(layer_l.parameters(), lr=lr) for layer_l in self.layers]
            self.back_opts = [torch.optim.Adam(layer_l.parameters(), lr=lr) for layer_l in self.back_layers]
        if training_method == "difference_target_propagation":
            self.back_layers = nn.ModuleList([])
            for out_l, in_l in zip(self.in_features[:-1], self.out_features[:-1]):
                self.back_layers.append(nn.ConvTranspose2d(in_l, out_l, kernel_size, padding=self.pad))
            self.back_layers.append(nn.Linear(self.out_features[-1], self.in_features[-1]))
            self.opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.layers]
            self.back_opts = [torch.optim.Adam(layer_l.parameters(), ) for layer_l in self.back_layers]

    def forward(self, x):
        for l in range(len(self.layers) - 1):
            x = self.layers[l](x)
            x = self.activation_fn(x)
            if l % 2:
                x = x[..., ::2, ::2]
                if l < len(self.batch_norms):
                    x = self.batch_norms[l // 2](x)
        x = torch.mean(x, dim=(2, 3))
        yhat = self.layers[-1](x)
        return yhat

    def forward_ls(self, x):
        yhats = []
        for l in range(len(self.layers) - 1):
            x = self.layers[l](x)
            x = self.activation_fn(x)
            x_mu = torch.mean(x, dim=(2, 3))
            yhat_l = self.ls_layers[l](x_mu)
            yhats.append(yhat_l)
            if l % 2:
                x = x[..., ::2, ::2]
                if l < len(self.batch_norms):
                    x = self.batch_norms[l // 2](x)
            x = torch.detach(x)
        x = torch.mean(x, dim=(2, 3))
        yhat = self.layers[-1](x)
        yhats.append(yhat)
        return yhats

    def forward_ff(self, xy):
        goodnesses = []
        for l in range(len(self.layers) - 1):
            xy = xy / (xy.norm(dim=1, keepdim=True) + 0.001)
            xy = self.layers[l](xy)
            xy = self.activation_fn(xy)
            goodness_l = xy.square().mean(dim=(1, 2, 3))
            goodnesses.append(goodness_l)
            if l % 2:
                xy = xy[..., ::2, ::2]
                if l < len(self.batch_norms):
                    xy = self.batch_norms[l // 2](xy)
            xy = torch.detach(xy)
        return goodnesses

    def forward_pc(self, x):
        local_losses = []
        for l in range(len(self.layers) - 1):
            x_out = self.layers[l](x)
            x_out = self.activation_fn(x_out)
            xhat = self.back_layers[l](x_out)
            xhat = self.activation_fn(xhat)
            loss_l = torch.mean((xhat - x) ** 2)
            local_losses.append(loss_l)
            x = x_out.detach()
            if l % 2:
                x = x[..., ::2, ::2]
                if l < len(self.batch_norms):
                    x = self.batch_norms[l // 2](x)
        x = torch.mean(x, dim=(2, 3))
        yhat = self.layers[-1](x)
        return local_losses, yhat

    def forward_dtp(self, x):

        h_list = []
        for l in range(len(self.layers) - 1):
            x = self.layers[l](x)
            x = self.activation_fn(x)
            h_list.append(x)
            x = x.detach()
            if l % 2:
                x = x[..., ::2, ::2]
                if l < len(self.batch_norms):
                    x = self.batch_norms[l // 2](x)
        out_shape = (1, 1) +  x.shape[2:]
        x = x.mean(dim=2)
        yhat = self.layers[-1](x)
        h_list.append(yhat)

        hhat_list = [None] * (len(h_list))
        hhat_list[-1] = h_list[-1].detach()
        for l in list(range(1, len(h_list))):
            h_curr = h_list[-l].detach()
            h_prev = h_list[-(l + 1)].detach()
            hhat_curr = hhat_list[-l].detach()
            ghhat_curr = self.back_layers[-l](hhat_curr)
            ghhat_curr = self.activation_fn(ghhat_curr)
            gh_curr = self.back_layers[-l](h_curr)
            gh_curr = self.activation_fn(gh_curr)
            if l == 1:
                gh_curr = gh_curr[..., None, None].repeat(out_shape)
                ghhat_curr = ghhat_curr[..., None, None].repeat(out_shape)
            if (l % 2) == 0:
                gh_curr = gh_curr.repeat_interleave(repeats=2, dim=2)
                ghhat_curr = ghhat_curr.repeat_interleave(repeats=2, dim=2)
                gh_curr = gh_curr.repeat_interleave(repeats=2, dim=3)
                ghhat_curr = ghhat_curr.repeat_interleave(repeats=2, dim=3)
            hhat_prev = h_prev + ghhat_curr - gh_curr
            hhat_list[-(l + 1)] = hhat_prev
        local_losses = []
        for l in range(len(h_list) - 1):
            loss_l = torch.mean((hhat_list[l] - h_list[l]) ** 2)
            local_losses.append(loss_l)

        return local_losses, yhat


def train_sgd_conv2d(X_train,
                     Y_train,
                     X_val,
                     Y_val,
                     hidden_dims,
                     activation,
                     training_method,
                     kernel_size=3,
                     patience=5,
                     max_epochs=50,
                     batch_size=5,
                     verbose=False,
                     loss_fn=torch.nn.CrossEntropyLoss(),
                     lr=0.001,
                     pad=0,
                     ):
    torch.cuda.empty_cache()
    start_time = time.perf_counter()

    activation_fn = activation_dict[activation]
    model = conv2dModel(
        in_features=X_train.shape[1],
        hidden_dims=hidden_dims,
        num_classes=Y_train.shape[1],
        activation_fn=activation_fn,
        kernel_size=kernel_size,
        training_method=training_method,
        pad=pad,
        lr=lr
    ).to(device)
    train_loss = []
    val_loss = []
    best_val_loss = torch.inf
    patience_counter = 0
    for epoch_i in range(max_epochs):
        train_loss_i = train_sgd(model=model,
                                 x=X_train,
                                 y=Y_train,
                                 loss_fn=loss_fn,
                                 batch_size=batch_size
                                 )
        train_loss.append(train_loss_i.item())
        val_loss_i = validate_sgd(model=model,
                                  x=X_val,
                                  y=Y_val,
                                  loss_fn=loss_fn,
                                  batch_size=batch_size)
        val_loss.append(val_loss_i.item())
        if verbose:
            print(val_loss_i)
        if val_loss_i < best_val_loss:
            best_val_loss = val_loss_i
            patience_counter = 0
        else:
            patience_counter += 1
        if patience_counter == (patience - 1):
            break

    end_time = time.perf_counter()
    training_time = end_time - start_time
    training_epochs = epoch_i

    return model, training_time, training_epochs


# </editor-fold>

# @title conv2d few-shot experiments (forward and sgd) on CXR and OCT

# <editor-fold desc="conv2d few-shot experiments (forward and SGD)">

sgd_training_methods = [
    "backprop",
    "local_supervision",
    "forward_forward",
    "predictive_coding",
    "difference_target_propagation",
]
all_training_methods = ["forward_projection", "random", "backprop", ]

experiment_parameters = expand_grid({
    'rep': list(range(10)),
    'n_sample': [20, 40]
})

seed = 0
random.seed(seed)
torch.manual_seed(seed)
np.random.seed(seed)
conv2d_experiments = []
for dataset_i in ['cxr', 'oct']:

    print(dataset_i)
    X_trainval, Y_trainval, X_test, Y_test, folds = load_dataset(dataset_i, channels_last=True)
    hidden_dim = 16
    n_blocks = 4
    hidden_dims = [hidden_dim * 2 ** (i // 2) for i in range(n_blocks * 2)]
    activation = "relu"
    train_folds = folds != 0
    val_folds = torch.logical_not(train_folds)
    X_train, X_val = X_trainval[train_folds], X_trainval[val_folds]
    Y_train, Y_val = Y_trainval[train_folds], Y_trainval[val_folds]

    for experiment_parameters_i in range(len(experiment_parameters)):

        print(dataset_i, experiment_parameters_i)

        rep = experiment_parameters.rep[experiment_parameters_i]
        n_sample = experiment_parameters.n_sample[experiment_parameters_i]

        try:
          for training_method in all_training_methods:

                n_train = round(n_sample * 0.8)
                n_val = n_sample - n_train

                X_train_s, Y_train_s = subsample_dataset(X_train,
                                                          Y_train,
                                                          n_sample=n_train)
                X_val_s, Y_val_s = subsample_dataset(X_train,
                                                      Y_train,
                                                      n_sample=n_val)

                if training_method in ["forward_projection", "random", "label_projection", "noisy_label_projection"]:

                    X_trainval_s = torch.concatenate([X_train_s, X_val_s])
                    Y_trainval_s = torch.concatenate([Y_train_s, Y_val_s])

                    w_list, _, _, training_time = train_forward_conv2d(x=X_trainval_s,
                                                                        y=Y_trainval_s,
                                                                        training_method=training_method,
                                                                        hidden_dim=hidden_dim,
                                                                        activation=activation,
                                                                        n_blocks=n_blocks,
                                                                        device=device,
                                                                        reg_factor=10,
                                                                        batch_size=5
                                                                        )
                    train_metrics = evaluate_forward_conv2d(x=X_trainval_s,
                                                            y=Y_trainval_s,
                                                            w_list=w_list,
                                                            activation=activation)
                    test_metrics = evaluate_forward_conv2d(x=X_test,
                                                            y=Y_test,
                                                            w_list=w_list,
                                                            activation=activation,
                                                            batch_size=50)


                else:

                    model, training_time, training_epochs = train_sgd_conv2d(
                        X_train=X_train_s.transpose(1, -1),
                        Y_train=Y_train_s,
                        X_val=X_val_s.transpose(1, -1),
                        Y_val=Y_val_s,
                        hidden_dims=hidden_dims,
                        activation=activation,
                        training_method=training_method,
                        lr=0.0001,
                        batch_size=25,
                        patience=10,
                        verbose=False)

                    train_metrics = evaluate_sgd(model=model,
                                                  x=X_train_s.transpose(1, -1),
                                                  y=Y_train_s,
                                                  )

                    val_metrics = evaluate_sgd(model=model,
                                                x=X_val_s.transpose(1, -1),
                                                y=Y_val_s,
                                                )

                    test_metrics = evaluate_sgd(model=model,
                                                x=X_test.transpose(1, -1),
                                                y=Y_test,
                                                )

                print(training_method, n_sample)
                print(train_metrics)
                print(test_metrics)

                out = {
                    'dataset': dataset_i,
                    'training_method': training_method,
                    'activation': activation,
                    'hidden_dim': hidden_dim,
                    'n_blocks': n_blocks,
                    'n_sample': n_sample,
                    'train_auc': train_metrics[0].item(),
                    'train_acc': train_metrics[1].item(),
                    'test_auc': test_metrics[0].item(),
                    'test_acc': test_metrics[1].item(),
                    'training_time': training_time,
                }
                conv2d_experiments.append(out)
        except:
          print("skipped")

conv2d_experiments = pd.DataFrame(conv2d_experiments)
output_file = os.path.join(output_dir, "conv2d_experiments.csv")
conv2d_experiments.to_csv(path_or_buf=output_file)

    # </editor-fold>

#@title forward conv2d cifar functions (with padding)


def ridge_regression_w_conv2d(x_batches, z_batches, reg_factor=10., device=device, reduce_factor=1):
    x_dim = x_batches[0].shape[-1] # data
    z_dim = z_batches[1].shape[-1] # targets

    #accumulate data gram matrix and cross product of data and targets
    gram_mat = torch.zeros((x_dim, x_dim), device=device)
    xt_z = torch.zeros((x_dim, z_dim), device=device)
    for x_i, z_i in zip(x_batches, z_batches):
        x_i = x_i.to(device)
        z_i = z_i.to(device)
        x_i = x_i.flatten(start_dim=0, end_dim=2)
        z_i = z_i.flatten(start_dim=0, end_dim=2)
        xt_z += (x_i.T @ z_i) * reduce_factor
        gram_mat += (x_i.T @ x_i) * reduce_factor

    #regularise
    gram_mat += torch.eye(gram_mat.shape[0], device=device) * reg_factor

    #invert gram matrix
    try:
        gram_inv = torch.inverse(gram_mat)
    except:
        print("singular gram matrix, consider increasing regularisation factor")
        gram_inv = torch.eye(gram_mat.shape[0], device=device)

    #compute weight matrix
    w_hat = gram_inv @ xt_z

    return w_hat


def fit_w_conv2d(x_batches,
                 y_batches,
                 hidden_dim=16,
                 reg_factor=0.01,
                 return_qu=False,
                 activation="relu",
                 device=device,
                 training_method="forward_projection",
                 reduce_factor=1,
                 ):
    x_channels = x_batches[0].shape[-1]
    y_channels = y_batches[0].shape[-1]

    q = torch.randn((x_channels, hidden_dim), device=device) # data projection matrix
    u = torch.randn((y_channels, hidden_dim), device=device) # label projection matrix

    z_batches = [] # targets
    for x_i, y_i in zip(x_batches, y_batches):

        x_i = x_i.to(device)
        y_i = y_i.to(device)

        #generate target values (z)
        y_proj = torch.sign(y_i @ u)
        match training_method:
            case "forward_projection":
                x_proj = torch.sign(x_i @ q)
            case "label_projection":
                x_proj = torch.zeros(x_i.shape[:-1] + (u.shape[-1],),
                                     device=device)
            case "noisy_label_projection":
                x_proj = torch.sign(torch.randn(x_i.shape[:-1] + (u.shape[-1],),
                                                device=device))

        z_i = x_proj + y_proj

        # transpose target distribution
        if activation_shift_dict[activation] != 0:
            z_i += activation_shift_dict[activation]
        if activation_rescale_dict[activation] != 1:
            z_i *= activation_rescale_dict[activation]
        z_batches.append(z_i.to("cpu"))

    #fit weight
    w = ridge_regression_w_conv2d(x_batches, z_batches, reg_factor=reg_factor, reduce_factor=reduce_factor)

    if return_qu:
        return w, q, u
    else:
        return w, None, None


def pad_array(x):
    x = torch.concatenate([x[:, 0,None, :, :], x, x[:, -1,None, :, :]], dim=1)
    x = torch.concatenate([x[:, :, 0,None, :], x, x[:, :, -1,None, :]], dim=2)
    return x

if True:
  def pad_array(x, pad_size=1):
      # x is expected to be channels last: (batch_size, height, width, channels)
      # Pad height (dimension 1)
      x = torch.concatenate([x[:, :pad_size, :, :], x, x[:, -pad_size:, :, :]], dim=1)
      # Pad width (dimension 2)
      x = torch.concatenate([x[:, :, :pad_size, :], x, x[:, :, -pad_size:, :]], dim=2)
      return x

def train_forward_conv2d(x,
                         y,
                         training_method,
                         activation='relu',
                         hidden_dim=16,
                         n_blocks=3,
                         kernel_size=3,
                         global_layer="average",
                         pad=False,
                         batch_size=25,
                         reg_factor=0.01,
                         reduce_factor=1,
                         return_qu=False,
                         verbose=False,
                         device=device,
                         ):
    activation_fn = activation_dict[activation]
    pad_size = kernel_size//2
    pad_tuple = (0,0,pad_size, pad_size, pad_size, pad_size, 0,0)

    if y.ndim == 2:
        y = y[:, None, None, :]

    hidden_dims = [round(hidden_dim * 2 ** (i // 2)) for i in range(n_blocks * 2)]

    start_time = time.perf_counter()
    w_list = [] # layer weight matrices
    q_list = [] # data projection matrices
    u_list = [] # label projeciton matrices

    rand_idx = torch.randperm(len(x))
    x_batches = list(torch.split(x[rand_idx], split_size_or_sections=batch_size))
    y_batches = list(torch.split(y[rand_idx], split_size_or_sections=batch_size))

    # fit hidden layers
    for l in range(len(hidden_dims)):

        if verbose:
            print('fitting layer', l)

        # pooling
        stride = 2 - ((l + 1) % 2)

        # convolution
        for i in range(len(x_batches)):
            x_i = x_batches[i]
            if pad:
                x_i = pad_array(x_i)
            x_i = x_i.unfold(dimension=1, size=kernel_size, step=stride)  #
            x_i = x_i.unfold(dimension=2, size=kernel_size, step=stride)  #
            x_i = x_i.flatten(start_dim=3)
            x_i = concatenate_ones(x_i)
            x_batches[i] = x_i

        # fitting hidden weights
        if training_method == "random":
            w = torch.randn((x_batches[0].shape[-1], hidden_dims[l])).to(device)
            w /= w.norm(dim=-1, keepdim=True)
        if training_method in ["forward_projection", "label_projection", "noisy_label_projection"]:
            w, q, u = fit_w_conv2d(x_batches,
                                   y_batches,
                                   hidden_dim=hidden_dims[l],
                                   return_qu=return_qu,
                                   activation=activation,
                                   training_method=training_method,
                                   reg_factor=reg_factor,
                                   reduce_factor=reduce_factor
                                   )
            q_list.append(q)
            u_list.append(u)
        w_list.append(w)

        # forward
        x_batches = [activation_fn(x_i.to(device) @ w).to("cpu") for x_i in x_batches]

    # fitting output layer
    if verbose:
        print('fitting output layer')
    for i in range(len(x_batches)):
        x_batches[i] = concatenate_ones(x_batches[i])
        if global_layer == "average":
            x_batches[i] = torch.mean(x_batches[i], dim=(1, 2), keepdim=True)
        elif global_layer == "flatten":
            x_batches[i] = x_batches[i].flatten(start_dim=1)[:, None, None, :]
        y_batches[i] = 2 * y_batches[i] - 1


    # fit weight
    w = ridge_regression_w_conv2d(x_batches, y_batches, reg_factor=1, reduce_factor=reduce_factor)

    w_list.append(w)
    end_time = time.perf_counter()
    training_time = end_time - start_time

    return w_list, q_list, u_list, training_time


def evaluate_forward_conv2d(x,
                            y,
                            w_list,
                            activation,
                            kernel_size=3,
                            pad=False,
                            global_layer="average",
                            batch_size=1000,

                            ):
    activation_fn = activation_dict[activation]
    pad_size = kernel_size // 2
    pad_tuple = (0,0,pad_size, pad_size, pad_size, pad_size, 0,0)
    x_batches = torch.split(x, split_size_or_sections=batch_size)

    yhat = []
    for x_i in x_batches:
        x_i = x_i.to(device)

        for l in range(len(w_list) - 1):
            if pad:
                #pad_size = kernel_size // 2
                x_i = pad_array(x_i)
                #x_i = torch.nn.functional.pad(x_i, pad=pad_tuple, mode='constant')

            # convolution and pooling
            stride = 2 - ((l + 1) % 2)
            x_i = x_i.unfold(dimension=1, size=kernel_size, step=stride)  #
            x_i = x_i.unfold(dimension=2, size=kernel_size, step=stride)  #

            x_i = x_i.flatten(start_dim=3)
            x_i = concatenate_ones(x_i)

            # forward
            x_i = activation_fn(x_i @ w_list[l])

        # output
        x_i = concatenate_ones(x_i)
        y = torch.squeeze(y)

        # extract global average as prediction
        match global_layer:
            case "average":
                x_i = torch.mean(x_i, dim=(1, 2))
            case "flatten":
                x_i = x_i.flatten(start_dim=1)
        yhat_i = x_i @ w_list[-1]
        yhat.append(yhat_i.to("cpu"))

    yhat = torch.concatenate(yhat)

    metrics = compute_metrics(yhat, y)

    return metrics

#@title conv2d cifar few shot experiments

# <editor-fold desc="conv2d cifar few-shot experiments (forward and SGD)">
sgd_training_methods = [
    "backprop",
    "local_supervision",
    "forward_forward",
    "predictive_coding",
    "difference_target_propagation",
]
all_training_methods = ["forward_projection", "random","backprop", "local_supervision"]

experiment_parameters = expand_grid({
    'rep': list(range(5)),
    'n_sample': [50, 100]
})

seed = 0
random.seed(seed)
torch.manual_seed(seed)
np.random.seed(seed)
conv2d_experiments = []
for dataset_i in ['CIFAR10']:

    print(dataset_i)
    X_trainval, Y_trainval, X_test, Y_test, folds = load_dataset(dataset_i, channels_last=True)
    hidden_dim = 32
    n_blocks = 4
    hidden_dims = [hidden_dim * 2 ** (i // 2) for i in range(n_blocks * 2)]
    activation = "relu"
    train_folds = folds != 0
    val_folds = torch.logical_not(train_folds)

    X_train, X_val = X_trainval[train_folds], X_trainval[val_folds]
    Y_train, Y_val = Y_trainval[train_folds], Y_trainval[val_folds]

    n_classes = 2
    selected_idx = Y_train[:, :n_classes].sum(dim=1) > 0
    X_train = X_train[selected_idx]
    Y_train = Y_train[selected_idx][:, :n_classes]
    selected_idx = Y_val[:, :n_classes].sum(dim=1) > 0
    X_val = X_val[selected_idx]
    Y_val = Y_val[selected_idx][:, :n_classes]
    selected_idx = Y_test[:, :n_classes].sum(dim=1) > 0
    X_test = X_test[selected_idx]
    Y_test = Y_test[selected_idx][:, :n_classes]

    for experiment_parameters_i in range(len(experiment_parameters)):

        print(dataset_i, experiment_parameters_i)

        rep = experiment_parameters.rep[experiment_parameters_i]
        n_sample = experiment_parameters.n_sample[experiment_parameters_i]

        for training_method in all_training_methods:

            n_train = round(n_sample * 0.8)
            n_val = n_sample - n_train

            X_train_s, Y_train_s = subsample_dataset(X_train,
                                                    Y_train,
                                                    n_sample=n_train)
            X_val_s, Y_val_s = subsample_dataset(X_train,
                                                Y_train,
                                                n_sample=n_val)

            if training_method in ["forward_projection", "random", ]:

                X_trainval_s = torch.concatenate([X_train_s, X_val_s])
                Y_trainval_s = torch.concatenate([Y_train_s, Y_val_s])

                w_list, _, _, training_time = train_forward_conv2d(x=X_trainval_s,
                                                                  y=Y_trainval_s,
                                                                  training_method=training_method,
                                                                  hidden_dim=hidden_dim,
                                                                  activation=activation,
                                                                  n_blocks=n_blocks,
                                                                  device=device,
                                                                  reg_factor=10,
                                                                  reduce_factor=0.01,
                                                                  batch_size=25,
                                                                  pad=True,
                                                                  verbose=False
                                                                  )
                train_metrics = evaluate_forward_conv2d(x=X_trainval_s,
                                                        y=Y_trainval_s,
                                                        w_list=w_list,
                                                        activation=activation,
                                                        batch_size=50,
                                                        pad=True,
                                                        )
                test_metrics = evaluate_forward_conv2d(x=X_test,
                                                      y=Y_test,
                                                      w_list=w_list,
                                                      activation=activation,
                                                      batch_size=50,
                                                        pad=True,)


            else:

                model, training_time, training_epochs = train_sgd_conv2d(
                    X_train=X_train_s.transpose(1, -1),
                    Y_train=Y_train_s,
                    X_val=X_val_s.transpose(1, -1),
                    Y_val=Y_val_s,
                    hidden_dims=hidden_dims,
                    activation=activation,
                    training_method=training_method,
                    lr=0.0001,
                    batch_size=25,
                    patience=10,
                    pad=1,
                    verbose=False)

                train_metrics = evaluate_sgd(model=model,
                                            x=X_train_s.transpose(1, -1),
                                            y=Y_train_s,
                                            )

                val_metrics = evaluate_sgd(model=model,
                                          x=X_val_s.transpose(1, -1),
                                          y=Y_val_s,
                                          )

                test_metrics = evaluate_sgd(model=model,
                                            x=X_test.transpose(1, -1),
                                            y=Y_test,
                                            )

            print(training_method, n_sample)
            print(train_metrics)
            print(test_metrics)

            out = {
                'dataset': 'CIFAR',
                'training_method': training_method,
                'activation': activation,
                'hidden_dim': hidden_dim,
                'n_blocks': n_blocks,
                'n_sample': n_sample,
                'train_auc': train_metrics[0].item(),
                'train_acc': train_metrics[1].item(),
                'test_auc': test_metrics[0].item(),
                'test_acc': test_metrics[1].item(),
                'training_time': training_time,
            }
            conv2d_experiments.append(out)




  conv2d_experiments = pd.DataFrame(conv2d_experiments)
  output_file = os.path.join(output_dir, "conv2d_cifar_experiments.csv")
  conv2d_experiments.to_csv(path_or_buf=output_file)

# </editor-fold>

#@title Forward ViT training and evaluation functions
# <editor-fold desc="Forward transformer training and evaluation functions">

# Step 1: Extract image patches
def extract_patches(x, patch_size):
    n, num_channels = x.shape[:2]
    # Reshape into patches (n, num_patches, patch_size*patch_size*num_channels)
    patches = x.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)
    patches = patches.contiguous().view(n, num_channels, -1, patch_size * patch_size)
    patches = patches.permute(0, 2, 1, 3).contiguous().view(n, -1, patch_size * patch_size * num_channels)
    return patches  # Shape: (n, num_patches, patch_size*patch_size*num_channels)

# Multi-head attention
def scaled_dot_product_attention(Q, K, V):
    scale_factor = torch.sqrt(torch.tensor(Q.size(-1), dtype=torch.float32))
    scores = torch.matmul(Q, K.transpose(-2, -1)) / scale_factor
    attention_weights = nn.functional.softmax(scores, dim=-1)
    attention_output = torch.matmul(attention_weights, V)
    return attention_output

# forward conv1d function with parameter to prespecify label projection
def fit_w_conv1d(x_batches,
                 y_batches,
                 hidden_dim=32,
                 reg_factor=10.,
                 u=None,
                 activation="relu",
                 device=device,
                 training_method="forward_projection",
                 return_qu=False, ):
    x_channels = x_batches[0].shape[-1]
    y_channels = y_batches[0].shape[-1]

    q = torch.randn((x_channels, hidden_dim), device=device)  # data projection matrix
    if u is None:
        u = torch.randn((y_channels, hidden_dim), device=device)  # label projection matrix
    z_batches = []  # batches of targets
    for x_i, y_i in zip(x_batches, y_batches):

        x_i = x_i.to(device)
        y_i = y_i.to(device)

        # project labels
        y_proj = torch.sign(y_i @ u)
        match training_method:
            case "forward_projection":
                x_proj = torch.sign(x_i @ q)
            case "label_projection":
                x_proj = torch.zeros(x_i.shape[:-1] + (u.shape[-1],),
                                     device=device)
            case "noisy_label_projection":
                x_proj = torch.sign(torch.randn(x_i.shape[:-1] + (u.shape[-1],),
                                                device=device))

        # generate target potentials (z)
        z_i = x_proj + y_proj

        # transpose distribution of labels
        if activation_shift_dict[activation] != 0:
            z_i += activation_shift_dict[activation]
        if activation_rescale_dict[activation] != 1:
            z_i *= activation_rescale_dict[activation]
        z_batches.append(z_i.to("cpu"))

    # model target potentials
    w = ridge_regression_w_conv1d(x_batches, z_batches, reg_factor=reg_factor)

    if return_qu:
        return w, q, u
    else:
        return w, None, None


# multi-head attention fitting function
def fit_multi_head_attention(x_batches,
                             y_batches,
                             num_heads,
                             head_dim,
                             ):
    y_channels = y_batches[0].shape[-1]
    total_dim = head_dim * num_heads
    u = torch.randn((y_channels, total_dim), device=device)  # label projection matrix
    w_qkv = []
    for _ in range(3):
        # use different q projection matrices for fitting query key and value weights
        w_i, _, _ = fit_w_conv1d(x_batches=x_batches,
                                 y_batches=y_batches,
                                 hidden_dim=total_dim,
                                 u=u)
        w_i = w_i.reshape((w_i.shape[0], head_dim, num_heads))
        w_qkv.append(w_i)
    w_query, w_key, w_value = w_qkv

    return w_query, w_key, w_value


def multi_head_attention(x_batches,
                         w_query,
                         w_key,
                         w_value):
    num_heads = w_query.shape[-1]
    for i in range(len(x_batches)):
        x_i = x_batches[i]
        concatenated_heads = []
        for head_i in range(num_heads):
            Q = x_i @ w_query[..., head_i]
            K = x_i @ w_key[..., head_i]
            V = x_i @ w_value[..., head_i]
            attention_i = scaled_dot_product_attention(Q, K, V)
            concatenated_heads.append(attention_i)
        concatenated_heads = torch.cat(concatenated_heads, dim=-1)
        x_batches[i] = concatenated_heads
    return x_batches

def add_positional_encoding(x, positional_encoding=None):
    pos_encoding_long = positional_encoding.repeat(x.shape[0], 1, 1).to(device)
    x = torch.concatenate([x, pos_encoding_long], dim=-1)
    return x  # Shape: (batch_size, num_patches, embed_dim+1)


# Step 6: Add and Normalize layers
def add_and_normalize(x, y):
    return nn.functional.layer_norm(x + y, x.size())


def train_forward_transformer(x,
                              y,
                              patch_size,
                              num_heads,
                              embed_dim=128,
                              mlp_dim=128,
                              n_attn_layers=1,
                              batch_size=100,
                              activation="relu",
                              global_layer="average",
                              reg_factor=10.,
                              training_method="forward_projection",
                              verbose=False,
                              device=device):
    """
    Train a transformer model using ridge regression for weights.

    Args:
        x (torch.Tensor): Input tensor of shape (n_samples, height, width, channels).
        y (torch.Tensor): Target tensor of shape (n_samples, num_classes).
        patch_size (int): Size of patches to extract.
        num_heads (int): Number of attention heads.
        head_dim (int): Dimension of each attention head.
        n_attn_layers (int): Number of attention layers.
        mlp_dim (int): Dimension of the MLP layer.
        batch_size (int): Batch size for training.
        activation (str): Activation function to use.
        reg_factor (float): Regularization factor for ridge regression.
        return_qu (bool): Whether to return projection matrices.
        verbose (bool): Whether to print progress.
        device (str): Device to use for training.

    Returns:
        dict: Dictionary containing trained weights and training time.
    """
    with torch.no_grad():
        if y.ndim == 2:
            y = torch.unsqueeze(y, dim=1)

        start_time = time.perf_counter()
        activation_fn = activation_dict[activation]
        height, width = x.shape[1:3]
        num_patches_height = height // patch_size

        # Step 1: Extract patches
        x = x.permute(0, 3, 1, 2)  # Convert to (n, channels, height, width)
        x = extract_patches(x, patch_size)

        # Step 2: Add positional encoding
        positional_encoding = torch.linspace(-1, 1, num_patches_height)[:, None].repeat((1, num_patches_height))
        positional_encoding = torch.stack([positional_encoding.T, positional_encoding])
        positional_encoding = positional_encoding.flatten(start_dim=1).T[None, ...]
        x = add_positional_encoding(x, positional_encoding)

        # Step 3: Split into batches
        x_batches = list(torch.split(x, split_size_or_sections=batch_size))
        y_batches = list(torch.split(y, split_size_or_sections=batch_size))

        # Step 4: Fit embedding layer
        w_embedding, _, _ = fit_w_conv1d(x_batches,
                                         y_batches,
                                         hidden_dim=embed_dim,
                                         activation="relu")
        x_batches = [activation_fn(x_i @ w_embedding) for x_i in x_batches]

        # Initialize weights
        w_query_list, w_key_list, w_value_list, w_mlp_list = [], [], [], []

        # Step 5: Fit attention layers
        for layer in range(n_attn_layers):
            if verbose:
                print(f"Training attention layer {layer + 1}/{n_attn_layers}")

            # Fit multi-head attention weights
            if training_method == "random":
                w_query = torch.randn((x_batches[0].shape[-1], num_heads, head_dim), device=device)
                w_key = torch.randn((x_batches[0].shape[-1], num_heads, head_dim), device=device)
                w_value = torch.randn((x_batches[0].shape[-1], num_heads, head_dim), device=device)
                w_query /= w_query.norm(dim=-1, keepdim=True)
                w_key /= w_key.norm(dim=-1, keepdim=True)
                w_value /= w_value.norm(dim=-1, keepdim=True)

            if training_method == "forward_projection":
                w_query, w_key, w_value = fit_multi_head_attention(
                    x_batches, y_batches, num_heads, head_dim
                )
            w_query_list.append(w_query)
            w_key_list.append(w_key)
            w_value_list.append(w_value)

            # Apply multi-head attention
            x_batches = multi_head_attention(x_batches, w_query, w_key, w_value)

            # Fit MLP weights
            x_batches = [concatenate_ones(x_i) for x_i in x_batches]
            if training_method == "random":
                w_mlp = torch.randn((x_batches[0].shape[-1], mlp_dim), device=device)
                w_mlp /= w_mlp.norm(dim=-1, keepdim=True)
            if training_method == "forward_projection":
                w_mlp, _, _ = fit_w_conv1d(x_batches, y_batches, hidden_dim=mlp_dim, activation=activation)
            w_mlp_list.append(w_mlp)
            x_batches = [activation_fn(x_i @ w_mlp) for x_i in x_batches]

        # Step 6: Fit output layer
        if verbose:
            print("Fitting output layer")
        for i in range(len(x_batches)):
            x_batches[i] = concatenate_ones(x_batches[i])
            if global_layer == "flatten":
                x_batches[i] = x_batches[i].flatten(start_dim=1)[:, None, :]
            else:
                x_batches[i] = torch.mean(x_batches[i], dim=1, keepdim=True)
            y_batches[i] = 2 * y_batches[i] - 1
        w_out = ridge_regression_w_conv1d(x_batches, y_batches, reg_factor=reg_factor)

        end_time = time.perf_counter()
        training_time = end_time - start_time

        w_list = [w_embedding, w_query_list, w_key_list, w_value_list, w_mlp_list, w_out]

        # Return weights and training time
        return w_list, training_time


def evaluate_forward_transformer(x,
                                 y,
                                 w_list,
                                 patch_size,
                                 activation="relu",
                                 batch_size=100,
                                 global_layer="average",
                                 device="cpu"):
    """
    Evaluate a trained transformer model.

    Args:
        x (torch.Tensor): Input tensor of shape (n_samples, height, width, channels).
        y (torch.Tensor): Target tensor of shape (n_samples, num_classes).
        w_query_list (list): List of query weight matrices for each attention layer.
        w_key_list (list): List of key weight matrices for each attention layer.
        w_value_list (list): List of value weight matrices for each attention layer.
        w_mlp_list (list): List of MLP weight matrices for each layer.
        w_out (torch.Tensor): Output layer weight matrix.
        patch_size (int): Size of patches to extract.
        activation (str): Activation function to use.
        batch_size (int): Batch size for evaluation.
        global_layer (str): Global pooling method ("average" or "flatten").
        device (str): Device to use for evaluation.

    Returns:
        torch.Tensor: Evaluation metrics.
    """
    w_embedding, w_query_list, w_key_list, w_value_list, w_mlp_list, w_out = w_list

    with torch.no_grad():
        if y.ndim == 2:
            y = torch.unsqueeze(y, dim=1)

        start_time = time.perf_counter()
        activation_fn = activation_dict[activation]
        height, width = x.shape[1:3]
        num_patches_height = height // patch_size

        # Step 1: Extract patches
        x = x.permute(0, 3, 1, 2)  # Convert to (n, channels, height, width)
        x = extract_patches(x, patch_size)

        # Step 2: Add positional encoding
        positional_encoding = torch.linspace(-1, 1, num_patches_height)[:, None].repeat((1, num_patches_height))
        positional_encoding = torch.stack([positional_encoding.T, positional_encoding])
        positional_encoding = positional_encoding.flatten(start_dim=1).T[None, ...]
        x = add_positional_encoding(x, positional_encoding)

        # Step 3: Split into batches
        x_batches = list(torch.split(x, split_size_or_sections=batch_size))
        y_batches = list(torch.split(y, split_size_or_sections=batch_size))

        # Step 4: embedding layer
        x_batches = [activation_fn(x_i @ w_embedding) for x_i in x_batches]

        # Step 5: attention layers
        n_attn_layers = len(w_query_list)
        for layer in range(n_attn_layers):
            # Apply multi-head attention
            w_query = w_query_list[layer]
            w_key = w_key_list[layer]
            w_value = w_value_list[layer]
            x_batches = multi_head_attention(x_batches, w_query, w_key, w_value)

            # Apply MLP weights
            x_batches = [concatenate_ones(x_i) for x_i in x_batches]
            w_mlp = w_mlp_list[layer]
            x_batches = [activation_fn(x_i @ w_mlp) for x_i in x_batches]

        # Step 6: Apply output layer
        for i in range(len(x_batches)):
            x_batches[i] = concatenate_ones(x_batches[i])
            if global_layer == "flatten":
                x_batches[i] = x_batches[i].flatten(start_dim=1)[:, None, :]
            else:
                x_batches[i] = torch.mean(x_batches[i], dim=1, keepdim=True)
            y_batches[i] = 2 * y_batches[i] - 1
        yhat = [x_i @ w_out for x_i in x_batches]
        yhat = torch.concatenate(yhat)
        metrics = compute_metrics(yhat, y)

    return metrics


# </editor-fold>

#@title Forward ViT experiments
# <editor-fold desc="Forward ViT experiments">
model_parameters = expand_grid({
    'fold': list(range(5)),
    'training_method': ["forward_projection", "random", ],
    'activation': ["relu"],
})
verbose = True

seed = 0
random.seed(seed)
torch.manual_seed(seed)
np.random.seed(seed)
forward_conv1d_experiments = []
dataset_i = "CIFAR10"
n_classes = 2
print(dataset_i)
X_trainval, Y_trainval, X_test, Y_test, folds = load_dataset(dataset_i)
selected_idx = Y_trainval[:, :n_classes].any(dim=(1,))
X_trainval = X_trainval[selected_idx]
Y_trainval = Y_trainval[selected_idx][:, :n_classes]
folds = folds[selected_idx]
selected_idx = Y_test[:, :n_classes].any(dim=(1,))
X_test = X_test[selected_idx]
Y_test = Y_test[selected_idx][:, :n_classes]

# Set parameters
verbose = True
batch_size = 50
patch_size = 4
num_heads = 8
embed_dim = 64
head_dim = embed_dim // num_heads
n_ViT_layers = 4
mlp_dim = 64
global_layer = "average"
attn_softmax = True
experiments = []
for model_parameters_i in range(len(model_parameters)):

    print(model_parameters_i)

    training_method = model_parameters.training_method[model_parameters_i]
    fold = model_parameters.fold[model_parameters_i]
    activation = model_parameters.activation[model_parameters_i]

    train_folds = folds != fold
    val_folds = torch.logical_not(train_folds)
    X_train, X_val = X_trainval[train_folds], X_trainval[val_folds]
    Y_train, Y_val = Y_trainval[train_folds], Y_trainval[val_folds]

    w_list, training_time = train_forward_transformer(x=X_trainval,
                                                      y=Y_trainval,
                                                      training_method=training_method,
                                                      activation=activation,
                                                      n_attn_layers=n_ViT_layers,
                                                      device=device,
                                                      batch_size=10,
                                                      patch_size=patch_size,
                                                      num_heads=num_heads,
                                                      global_layer=global_layer
                                                      )
    train_metrics = evaluate_forward_transformer(x=X_train,
                                                 y=Y_train,
                                                 w_list=w_list,
                                                 patch_size=patch_size,
                                                 device=device,
                                                 activation=activation,
                                                 global_layer=global_layer)
    val_metrics = evaluate_forward_transformer(x=X_val,
                                               y=Y_val,
                                               w_list=w_list,
                                               patch_size=patch_size,
                                               device=device,
                                               activation=activation,
                                               global_layer=global_layer)
    test_metrics = evaluate_forward_transformer(x=X_test,
                                                y=Y_test,
                                                w_list=w_list,
                                                patch_size=patch_size,
                                                device=device,
                                                activation=activation,
                                                global_layer=global_layer)
    if verbose:
        print(training_method)
        print(train_metrics)
        print(val_metrics)
        print(test_metrics)

    out = {
        'dataset': "CIFAR",
        'training_method': training_method,
        'fold': fold,
        'activation': activation,
        "embed_dim" : embed_dim,
        "mlp_dim" : mlp_dim,
        "n_ViT_layers" : n_ViT_layers,
        "num_heads" : num_heads,
        "patch_size" : patch_size,
        'train_auc': train_metrics[0].item(),
        'train_acc': train_metrics[1].item(),
        'train_prec': train_metrics[2].item(),
        'train_recall': train_metrics[3].item(),
        'train_f1': train_metrics[4].item(),
        'val_auc': val_metrics[0].item(),
        'val_acc': val_metrics[1].item(),
        'val_prec': val_metrics[2].item(),
        'val_recall': val_metrics[3].item(),
        'val_f1': val_metrics[4].item(),
        'test_auc': test_metrics[0].item(),
        'test_acc': test_metrics[1].item(),
        'test_prec': test_metrics[2].item(),
        'test_recall': test_metrics[3].item(),
        'test_f1': test_metrics[4].item(),
        'training_time': training_time,
    }
    experiments.append(out)
experiments = pd.DataFrame(experiments)
output_file = os.path.join(output_dir, "forward_vit_experiments.csv")
experiments.to_csv(path_or_buf=output_file)

# </editor-fold>

#@title SGD ViT training and evaluation functions
# <editor-fold desc="SGD ViT training and evaluation functions">

class PatchEmbedding(nn.Module):
    def __init__(self, img_size, patch_size, in_channels, embed_dim):
        super().__init__()
        self.n_patches = (img_size // patch_size) ** 2
        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        x = self.proj(x)  # (B, embed_dim, H/patch, W/patch)
        x = x.flatten(2)  # (B, embed_dim, N_patches)
        x = x.transpose(1, 2)  # (B, N_patches, embed_dim)
        return x

class ViTLayer(nn.Module):
    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.):
        super().__init__()
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, mlp_dim),
            nn.ReLU(),
            nn.Linear(mlp_dim, embed_dim),
        )

    def forward(self, x):
        # Self-attention block
        x2 = self.norm1(x)
        attn_out, _ = self.attn(x2, x2, x2)
        x = x + attn_out
        # MLP block
        x2 = self.norm2(x)
        x = x + self.mlp(x2)
        return x

def create_positional_encoding(x, patch_size):
    num_patches_height = x.shape[2] // patch_size
    positional_encoding = torch.linspace(-1, 1, num_patches_height)[:, None].repeat((1, num_patches_height))
    positional_encoding = torch.stack([positional_encoding.T, positional_encoding])
    positional_encoding = positional_encoding.flatten(start_dim=1).T[None, ...]

    return positional_encoding


class ViTPositionalEncoding(nn.Module):
    def __init__(self, num_patches, embed_dim):
        super().__init__()
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
        nn.init.trunc_normal_(self.pos_embed, std=0.02)

    def forward(self, x):
        # x: (batch_size, num_patches, embed_dim)
        return x + self.pos_embed

class ViTModel(nn.Module):
    def __init__(self,
                 embed_dim=64,
                 mlp_dim=64,
                 n_ViT_layers=4,
                 patch_size=4,
                 num_heads=8,
                 in_channels=3,
                img_size=32,
                 n_classes=2,
                 training_method="backprop",
                 ):
        super().__init__()
        self.patch_size = patch_size
        self.in_channels = in_channels
        self.training_method = training_method
        self.n_classes = n_classes
        self.pos_encoding = ViTPositionalEncoding((img_size // patch_size) ** 2, embed_dim)
        self.patch_embedding = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)
        self.vit_layers = [ViTLayer(embed_dim, num_heads, mlp_dim) for _ in range(n_ViT_layers)]
        self.output_layer = nn.Linear(embed_dim, n_classes)
        self.all_layers = nn.ModuleList(self.vit_layers + [self.output_layer])
        self.opt = torch.optim.Adam(self.all_layers.parameters())


    def forward(self, x):
        x = self.patch_embedding(x)
        x = self.pos_encoding(x)
        for layer_l in self.vit_layers:
            x = layer_l(x)
        x = torch.mean(x, dim=1)
        yhat = self.output_layer(x)
        return yhat

def train_sgd_vit(X_train,
                     Y_train,
                     X_val,
                     Y_val,
                     patience=5,
                     max_epochs=25,
                     batch_size=25,
                  embed_dim=64,
                 mlp_dim=64,
                  num_heads=8,
                 n_ViT_layers=4,
                  patch_size=4,
                     verbose=False,
                     device=device,
                     training_method="backprop",
                     loss_fn=torch.nn.CrossEntropyLoss()):
    torch.cuda.empty_cache()
    start_time = time.perf_counter()

    model = ViTModel(
        n_classes=Y_train.shape[1],
        num_heads=num_heads,
        patch_size=patch_size,
        embed_dim=embed_dim,
        mlp_dim=mlp_dim,
        n_ViT_layers=n_ViT_layers,
        training_method=training_method,
    ).to(device)
    train_loss = []
    val_loss = []
    best_val_loss = torch.inf
    patience_counter = 0
    for epoch_i in range(max_epochs):
        train_loss_i = train_sgd(model=model,
                                 x=X_train,
                                 y=Y_train,
                                 loss_fn=loss_fn,
                                 batch_size=batch_size
                                 )
        train_loss.append(train_loss_i.item())
        val_loss_i = validate_sgd(model=model,
                                  x=X_val,
                                  y=Y_val,
                                  loss_fn=loss_fn,
                                  batch_size=batch_size)
        val_loss.append(val_loss_i.item())
        if verbose:
            print(val_loss_i)
        if val_loss_i < best_val_loss:
            best_val_loss = val_loss_i
            patience_counter = 0
        else:
            patience_counter += 1
        if patience_counter == (patience - 1):
            break

    end_time = time.perf_counter()
    training_time = end_time - start_time
    training_epochs = epoch_i

    model = ViTModel().to(device)
    print("final training")
    X_trainval = torch.concatenate([X_train, X_val])
    Y_trainval = torch.concatenate([Y_train, Y_val])
    for _ in range(epoch_i):
        _ = train_sgd(model=model,
                      x=X_trainval,
                      y=Y_trainval,
                      loss_fn=loss_fn,
                      batch_size=batch_size)

    return model, training_time, training_epochs

# </editor-fold>

#@title SGD ViT experiments
# <editor-fold desc="SGD ViT experiments">

model_parameters = expand_grid({
    'fold': list(range(5)),
    'activation': ["relu"],
    'training_method': ["backprop",],
})

verbose = False

seed = 0
random.seed(seed)
torch.manual_seed(seed)
np.random.seed(seed)
experiments = []
dataset_i = "CIFAR10"

print(dataset_i)
X_trainval, Y_trainval, X_test, Y_test, folds = load_dataset(dataset_i,
                                                              channels_last=False)
n_classes
selected_idx = Y_trainval[:, :n_classes].any(dim=(1,))
X_trainval = X_trainval[selected_idx]
Y_trainval = Y_trainval[selected_idx][:, :n_classes]
folds = folds[selected_idx]
selected_idx = Y_test[:, :n_classes].any(dim=(1,))
X_test = X_test[selected_idx]
Y_test = Y_test[selected_idx][:, :n_classes]

for model_parameters_i in range(len(model_parameters)):
    print(model_parameters_i)

    fold = model_parameters.fold[model_parameters_i]
    activation = model_parameters.activation[model_parameters_i]
    training_method = model_parameters.training_method[model_parameters_i]
    embed_dim = 64
    mlp_dim = 64
    n_ViT_layers = 4
    num_heads = 8
    patch_size = 4

    train_folds = folds != fold
    val_folds = torch.logical_not(train_folds)
    X_train, X_val = X_trainval[train_folds], X_trainval[val_folds]
    Y_train, Y_val = Y_trainval[train_folds], Y_trainval[val_folds]

    model, training_time, training_epochs = train_sgd_vit(
        X_train=X_train,
        Y_train=Y_train,
        X_val=X_val,
        Y_val=Y_val,
        max_epochs=50,
        batch_size=25,
        patch_size=patch_size,
        embed_dim=embed_dim,
        mlp_dim=mlp_dim,
        n_ViT_layers=n_ViT_layers,
        num_heads=num_heads,
        verbose=True)

    train_metrics = evaluate_sgd(model=model,
                                  x=X_train,
                                  y=Y_train,
                                  )

    val_metrics = evaluate_sgd(model=model,
                                x=X_val,
                                y=Y_val,
                                )

    test_metrics = evaluate_sgd(model=model,
                                x=X_test,
                                y=Y_test,
                                )
    if verbose:
        print(training_method, dataset_i, activation, sep="\n")
        print(train_metrics)
        print(val_metrics)
        print(test_metrics)

    out = {
        'dataset': "CIFAR",
        'training_method': training_method,
        'fold': fold,
        'activation': activation,
        "embed_dim" : embed_dim,
        "mlp_dim" : mlp_dim,
        "n_ViT_layers" : n_ViT_layers,
        "num_heads" : num_heads,
        "patch_size" : patch_size,
        'train_auc': train_metrics[0].item(),
        'train_acc': train_metrics[1].item(),
        'train_prec': train_metrics[2].item(),
        'train_recall': train_metrics[3].item(),
        'train_f1': train_metrics[4].item(),
        'val_auc': val_metrics[0].item(),
        'val_acc': val_metrics[1].item(),
        'val_prec': val_metrics[2].item(),
        'val_recall': val_metrics[3].item(),
        'val_f1': val_metrics[4].item(),
        'test_auc': test_metrics[0].item(),
        'test_acc': test_metrics[1].item(),
        'test_prec': test_metrics[2].item(),
        'test_recall': test_metrics[3].item(),
        'test_f1': test_metrics[4].item(),
        'training_time': training_time,
        'training_epochs': training_epochs,
    }
    experiments.append(out)

experiments = pd.DataFrame(experiments)
output_file = os.path.join(output_dir, f"sgd_vit_experiments.csv")
experiments.to_csv(path_or_buf=output_file)

# </editor-fold>

#@title Organising results
# <editor-fold desc="Organising results">

training_method_names = {
    "forward_projection": "FP",
    "random": "RF",
    "label_projection": "LP",
    "noisy_label_projection": "LPN",
    "local_supervision": "LS",
    "forward_forward": "FF",
    "predictive_coding": "PC",
    "difference_target_propagation": "DTP",
    "backprop": "BP",
}

pd.options.mode.copy_on_write = True


#conv2d few shot experiments
conv2d_experiments_file1 = os.path.join(output_dir, "conv2d_experiments.csv")
conv2d_experiments1 = pd.read_csv(conv2d_experiments_file1, index_col=0)
conv2d_experiments_file2 = os.path.join(output_dir, "conv2d_cifar_experiments.csv")
conv2d_experiments2 = pd.read_csv(conv2d_experiments_file2, index_col=0)
all_tables = pd.concat([conv2d_experiments1, conv2d_experiments2], axis=0, ignore_index=True)


all_tables.columns = all_tables.columns.str.replace("_", " ")
all_tables.columns = all_tables.columns.str.title()
all_tables.columns = [x.replace("Training Method", "Method") for x in all_tables.columns]
all_tables.columns = [x.replace("Auc", "AUC") for x in all_tables.columns]
all_tables.Method = all_tables.Method.replace(training_method_names)
all_tables.Method = pd.Categorical(all_tables.Method,
                                  categories=training_method_names.values())
all_tables.Dataset = pd.Categorical(all_tables.Dataset,
                                    categories={"cxr": "CXR", "oct": "OCT", "CIFAR":"CIFAR"})
n_sample_values = all_tables['N Sample'].unique().tolist()
n_sample_values.sort()
print(n_sample_values)
all_tables['N Sample'] = "N=" + all_tables['N Sample'].astype(str)
all_tables['N Sample'] = pd.Categorical(all_tables['N Sample'], categories=["N=" + str(i) for i in n_sample_values])

#other tables
all_perf = all_tables.copy()
all_perf = all_perf.loc[all_perf.Method.isin(["FP", "RF", "LS", "FF",'PC', 'DTP',  "BP"]), :]
all_perf = all_perf = all_perf.drop(labels=["Activation"], axis=1)
all_perf = all_perf.groupby(['Dataset', 'Method', "N Sample"], observed=True).aggregate(mean_sd_func2)[["Train AUC", "Test AUC"]]
all_perf = pd.melt(all_perf, ignore_index=False,
                  var_name='Metric')
all_perf.reset_index(inplace=True)
all_perf.Metric = pd.Categorical(all_perf.Metric, categories=["Train AUC", "Test AUC"])
all_perf = all_perf.pivot(index=["Dataset",  "Metric", "N Sample"],
                          columns="Method",
                          values=["value"]
                          )
all_perf.to_csv(os.path.join(output_dir, "conv2d_fewshot_test_performance.csv"))

results_files = [
  "forward_conv1d_experiments.csv",
  "sgd_conv1d_experiments.csv",
  "forward_vit_experiments.csv",
  "sgd_vit_experiments.csv"
              ]
results_files = [os.path.join(output_dir, i) for i in results_files]

results_tables = [pd.read_csv(i, index_col=0, header=0) for i in results_files]
conv1d_tables = [results_tables[i] for i in range(len(results_files)) if "conv1d" in results_files[i]]
conv1d_tables[0]["training_epochs"] = 1
conv1d_tables = pd.concat(conv1d_tables)
conv1d_tables = conv1d_tables.drop(['hidden_dim', 'n_blocks'], axis=1)
vit_tables = [results_tables[i] for i in range(len(results_files)) if "vit" in results_files[i]]
vit_tables = pd.concat(vit_tables)
vit_tables = vit_tables.loc[:, conv1d_tables.columns]
vit_tables.dataset = vit_tables.dataset.replace({"CIFAR": "CIFAR"})


all_tables = pd.concat([conv1d_tables, vit_tables])

all_tables.columns = all_tables.columns.str.replace("_", " ")
all_tables.columns = all_tables.columns.str.title()
all_tables.columns = [x.replace("Training Method", "Method") for x in all_tables.columns]
all_tables.columns = [x.replace("Test ", "") for x in all_tables.columns]
all_tables.columns = [x.replace("Auc", "AUC") for x in all_tables.columns]
all_tables.Method = all_tables.Method.replace(training_method_names)
all_tables.Method = pd.Categorical(all_tables.Method,
                                  categories=training_method_names.values())

all_tables.Dataset = all_tables.Dataset.replace({"ptbxl_mi": "PTBXL-MI", "human_nontata_promoters": "Promoters"})
all_tables = all_tables.loc[all_tables.Dataset.isin(['FashionMNIST', 'Promoters', 'PTBXL-MI', 'CIFAR'])].reset_index()
all_tables.Dataset = pd.Categorical(all_tables.Dataset, categories=['Promoters', 'CIFAR'])

#relu activated performance
all_perf = all_tables.copy()
all_perf = all_perf.loc[all_tables.Activation == "relu"]
all_perf = all_perf.loc[all_perf.Method.isin(['FP', 'RF', 'LS', 'FF','PC', 'DTP',  'BP'])]
all_perf = all_perf.drop(labels=["Activation"], axis=1)
all_perf = all_perf.groupby(['Dataset', 'Method', ], observed=True).aggregate(mean_sd_func2)[['Acc', "AUC", ]]
all_perf = pd.melt(all_perf, ignore_index=False,
                  var_name='Metric')
all_perf.reset_index(inplace=True)
all_perf = all_perf.pivot(index=["Dataset", "Metric", ],
                          columns="Method",
                          values=["value"]
                          )
all_perf.to_csv(os.path.join(output_dir, f"main_test_performance.csv"))





# </editor-fold>